{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from scipy.misc import imread, imresize\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "#         resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        resnet = torchvision.models.resnet(pretrained=True)\n",
    "        all_modules = list(resnet.children())\n",
    "        modules = all_modules[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.fine_tune()\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward function\n",
    "        input: - images : with shape (batch_size, 3, 224, 224) channel first\n",
    "        \"\"\"\n",
    "        # get the images features\n",
    "        encoded_image = self.resnet(images) # (batch_size, 2048, 7, 7)\n",
    "        \n",
    "        batch_size = encoded_image.shape[0]\n",
    "        features = encoded_image.shape[1]\n",
    "        num_pixels = encoded_image.shape[2] * encoded_image.shape[3]\n",
    "        # get the global feature by using average pooling and rsshape it to batch_size, 2048 (14)\n",
    "        global_features = self.avgpool(encoded_image).view(batch_size, -1) # (batch_size, 2048)\n",
    "        # get the encoded image by resize the image feature\n",
    "        enc_image = encoded_image.permute(0, 2, 3, 1) # (batch_size, 7, ,7, 2048)\n",
    "        enc_image = enc_image.view(batch_size, num_pixels, features) # (batch_size, num_pixels, 2048)\n",
    "        return enc_image, global_features\n",
    "    \n",
    "    def fine_tune(self, status=False):\n",
    "        if not status:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for module in list(self.resnet.children())[5:]: # last layer only, len total layer is 8\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AdaptiveLSTMCell, self).__init__()\n",
    "        # create LSTM cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        # create input gate\n",
    "        self.x_gate = nn.Linear(input_size, hidden_size)\n",
    "        # crate hidden gate\n",
    "        self.h_gate = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, states):\n",
    "        \"\"\"\n",
    "        Forward function for LSTM\n",
    "        input: - x : word token combined with encoded_image\n",
    "               - states : the old hidden cell and memory cell\n",
    "        \"\"\"\n",
    "        h_old, m_old = states\n",
    "        # do LSTM, and get new hidden and output\n",
    "        ht, mt = self.lstm_cell(x, (h_old, m_old))\n",
    "        # do sigmoid to the input and hidden to get visual sentinel St (9)\n",
    "        gt = F.sigmoid(self.x_gate(x) + self.h_gate(h_old))\n",
    "        # and then do tanh to get visual sentinel (10)\n",
    "        st = gt * F.tanh(mt)\n",
    "        return ht, mt, st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_dimension):\n",
    "        super(AdaptiveAttention, self).__init__()\n",
    "        self.sentinel_affine= nn.Linear(hidden_size, hidden_size)\n",
    "        self.sentinel_attention = nn.Linear(hidden_size, attention_dimension)\n",
    "        self.hidden_affine = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden_attention = nn.Linear(hidden_size, attention_dimension)\n",
    "        self.visual_attention = nn.Linear(hidden_size, attention_dimension)\n",
    "        self.alphas = nn.Linear(attention_dimension, 1)\n",
    "        self.context_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "   \n",
    "    def forward(self, spatial_image, decoder_output, st):\n",
    "        \"\"\"\n",
    "        Forward function for Adaptive Attention\n",
    "        input: - spatial_image : the spatial image with shape (batch_size, num_pixels, hidden_size)\n",
    "               - decoder_output : the decoder hidden state with shape (batch_size, hidden_size)\n",
    "               - st : Visual sentinel returned by sentinel class with shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # extract num_pixels\n",
    "        num_pixels = spatial_image.shape[1]\n",
    "        # get the visual attention using spatial_image as input\n",
    "        visual_attn = self.visual_attention(spatial_image) # (batch_size, num_pixels, att_dim)\n",
    "        # get sentinel affine using st as input with ReLU activation\n",
    "        sentinel_affine = F.relu(self.sentinel_affine(st)) # (batch_size, hidden_size)\n",
    "        # get sentinel attention using sentinel_affine as input\n",
    "        sentinel_attn = self.sentinel_attention(sentinel_affine) # (batch_size, att_dim)\n",
    "        \n",
    "        hidden_affine = F.tanh(self.hidden_affine(decoder_output)) # (batch_sizem hidden_size)\n",
    "        hidden_attn = self.hidden_attention(hidden_affine) # (batch_size, attention_dimension)\n",
    "        \n",
    "        hidden_resized = hidden_attn.unsqueeze(1).expand(hidden_attn.size(0), num_pixels + 1, hidden_attn.size(1))\n",
    "        \n",
    "        concat_features = torch.cat([spatial_image, sentinel_affine.unsqueeze(1)], dim=1) # (batch_size, num_pixels+1, hidden_size)\n",
    "        attended_features = torch.cat([visual_attn, sentinel_attn.unsqueeze(1)], dim=1) # (batch_size, num_pixels, attn_dim)\n",
    "        \n",
    "        # do tanh to attended and hidden (6)\n",
    "        attention = F.tanh(attended_features + hidden_resized) # (batch_size, num_pixles+1, attn_dim)\n",
    "        # do a forward linear layer\n",
    "        alpha = self.alphas(attention).squeeze(2) # (batch_size, num_pixels+1)\n",
    "        # and do softmax\n",
    "        att_weights = F.softmax(alpha, dim=1) # (batch_size, num_pixels+1)\n",
    "        \n",
    "        context = (concat_features * att_weights.unsqueeze(2)).sum(dim=1) # (batch_size, hidden_size)\n",
    "        # get the new beta value by getting the last value of att_weights\n",
    "        beta_value = att_weights[:, -1].unsqueeze(1) #(batch_size, 1)\n",
    "        \n",
    "        out_l = F.tanh(self.context_hidden(context + hidden_affine))\n",
    "        \n",
    "        return out_l, att_weights, beta_value\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, attention_dimension, embed_size, encoded_dimension):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.encoded_to_hidden = nn.Linear(encoded_dimension, hidden_size)\n",
    "        self.global_features = nn.Linear(encoded_dimension, embed_size)\n",
    "        # input of the LSTMCell should be of shape (batch_size, input_size)\n",
    "        # because the input and global features are concenated, then input_features should be embed_size*2\n",
    "        self.LSTM = AdaptiveLSTMCell(embed_size*2, hidden_size)\n",
    "        self.adaptive_attention = AdaptiveAttention(hidden_size, attention_dimension)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def init_hidden_state(self, encoded_image):\n",
    "        h = torch.zeros(encoded_image.shape[0], 512).to(device)\n",
    "        c = torch.zeros(encoded_image.shape[0], 512).to(device)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoded_image, global_features, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward function for decoder\n",
    "        input: - encoded_image : the encoded images from encoder with shape (batch_size, num_pixels, 2048)\n",
    "               - global_features : the global features from encoder with shape (batch_size, 2048)\n",
    "               - encoded_captions : encoded captions with shape (batch_size, max_caption_length)\n",
    "               - caption_lengths : encoded caption length with dimension (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # extract the batch size and num_pixels\n",
    "        batch_size = encoded_image.shape[0]\n",
    "        num_pixels = encoded_image.shape[1]\n",
    "        # get the spatial image\n",
    "        spatial_image = F.relu(self.encoded_to_hidden(encoded_image)) # (batch_size, num_pixels, hidden_size)\n",
    "        global_image = F.relu(self.global_features(global_features)) # (batch_size, embed_size)\n",
    "        # sort input data by decreasing length\n",
    "        # caption_length will contains the sorted length, and sort_idx will contains the sorted elements indices\n",
    "        caption_lengths, sort_idx = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        # sort spatial_image, global_image, encoded_captions and encoded_image batches by caption length\n",
    "        spatial_image = spatial_image[sort_idx]\n",
    "        global_image = global_image[sort_idx]\n",
    "        encoded_captions = encoded_captions[sort_idx]\n",
    "        encoded_image = encoded_image[sort_idx]\n",
    "        \n",
    "        # Embedding, each batch contains a caption. All batch have the same number of rows (words), since we previously\n",
    "        # padded the ones shorter than max_caption_lengths, as well as the same number of columns (embed_dimension)\n",
    "        embeddings = self.embedding(encoded_captions) # (batch_size, max_caption_length, embed_dimesion)\n",
    "        \n",
    "        # initialize LSTM\n",
    "        h, c = self.init_hidden_state(encoded_image) # (batch_size, hidden_size)\n",
    "        \n",
    "        # we won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        decode_lengths =(caption_lengths - 1).tolist()\n",
    "        \n",
    "        # create tensors to store word prediction score, alphas and betas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels+1).to(device)\n",
    "        betas = torch.zeros(batch_size, max(decode_lengths), 1).to(device)\n",
    "        \n",
    "        # concenate the embeddings and global image feature for LSTM input\n",
    "        global_image = global_image.unsqueeze(1).expand_as(embeddings)\n",
    "        inputs = torch.cat((embeddings, global_image), dim=2) # (batch_size, max_caption_length, embed_dimension * 2)\n",
    "        \n",
    "        # start decoding\n",
    "        for timestep in range(max(decode_lengths)):\n",
    "            # create a packet sequence to process the only effective batch size N_t at that timestep\n",
    "            batch_size_t = sum([l > timestep for l in decode_lengths])\n",
    "            current_input = inputs[:batch_size_t, timestep, :] # (batch_size_t, embed_dimension * 2)\n",
    "            # do LSTM\n",
    "            h, c, st = self.LSTM(current_input, (h[:batch_size_t], c[:batch_size_t])) # (batch_size, hidden_size)\n",
    "            # run the adaptive attention\n",
    "            out_l, alpha_t, beta_t = self.adaptive_attention(spatial_image[:batch_size_t], h, st)\n",
    "            # compute the probability over the vocabulary with fullt connected layer\n",
    "            pred = self.fc(self.dropout(out_l))\n",
    "            # store the prediction, alphas and betas value\n",
    "            predictions[:batch_size_t, timestep, :] = pred\n",
    "            alphas[:batch_size_t, timestep, :] = alpha_t\n",
    "            betas[:batch_size_t, timestep, :] = beta_t\n",
    "        return predictions, alphas, betas, encoded_captions, decode_lengths, sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(image, rev_word_map):\n",
    "    \"\"\"\n",
    "    predict output with beam size of 1 (predict the word and feet it to the next LSTM)\n",
    "    print out the generated sentence\n",
    "    \"\"\"\n",
    "    max_len = 20\n",
    "    sampled = []\n",
    "    img = imread(image)\n",
    "    img = imresize(img, (256, 256)) # (224, 224, 3)\n",
    "    img = img.transpose(2, 0, 1) # channel first (3, 224, 224)\n",
    "    img = img / 255. # normalize the input to 0 - 1\n",
    "    img = torch.FloatTensor(img).to(device) # convert to tensor\n",
    "    # normalize the input image\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img.to('cpu')).to(device)  # (3, 224, 224)\n",
    "    \n",
    "    # add single batch\n",
    "    image = image.unsqueeze(0) # (1, 3, 224, 224)\n",
    "    encoded_image, global_features = encoder(image)\n",
    "    num_pixels = encoded_image.shape[1]\n",
    "    spatial_image = F.relu(decoder.encoded_to_hidden(encoded_image)) # (batch_size, num_pixels, hidden_size)\n",
    "    global_image = F.relu(decoder.global_features(global_features)) # (batch_size, embed_size)\n",
    "    alphas = torch.zeros(max_len, num_pixels+1)\n",
    "    betas = torch.zeros(max_len, 1)\n",
    "    # create prediction with initial <start> token\n",
    "    predictions = torch.LongTensor([[word_map['<start>']]]).to(device) # (1, 1)\n",
    "    h, c = decoder.init_hidden_state(encoded_image)\n",
    "    \n",
    "    for timestep in range(max_len):\n",
    "        embeddings = decoder.embedding(predictions).squeeze(1) # (1, 1, embed_dim) --> (1, embed_dim)\n",
    "        inputs = torch.cat((embeddings, global_image), dim=1) # (1, embed_dim*2)\n",
    "        h, c, st = decoder.LSTM(inputs, (h, c))\n",
    "        out, alpha, beta = decoder.adaptive_attention(spatial_image, h, st)\n",
    "        pt = decoder.fc(out)\n",
    "        _, pred = pt.max(1)\n",
    "        sampled.append(pred.item())\n",
    "        alphas[timestep] = alpha\n",
    "        betas[timestep] = beta.item()\n",
    "    \n",
    "    generated_words = [rev_word_map[sampled[i]] for i in range(len(sampled))]\n",
    "    filtered_words = ' '.join([word for word in generated_words if word != '<end>'])\n",
    "    print(filtered_words)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from cococaptioncider.pycocotools.coco import COCO\n",
    "from cococaptioncider.pycocoevalcap.eval import COCOEvalCap\n",
    "from util import *\n",
    "from dataset import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameter\n",
    "embed_dim = 512 # dimension of word embeddings\n",
    "attention_dim = 512 # attention hidden size\n",
    "hidden_size = 512 # dimension of decoder LSTM\n",
    "cudnn.benchmark = True # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Training parameter\n",
    "start_epoch = 0\n",
    "epochs = 20\n",
    "epochs_since_improvement = 0\n",
    "batch_size = 100\n",
    "workers = 1\n",
    "encoder_lr = 1e-4\n",
    "decoder_lr = 5e-4\n",
    "grad_clip = 0.1\n",
    "best_cider = 0\n",
    "print_freq = 100\n",
    "fine_tune_encoder = False\n",
    "checkpoint = None\n",
    "annFile = 'cococaptioncider/annotations/new_indo_caption_val.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch, vocab_size):\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "    losses = AverageMeter()\n",
    "    top5accs = AverageMeter()\n",
    "    \n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "        # move to GPU\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "        \n",
    "        # Feed Forward\n",
    "        encoded_image, global_features = encoder(imgs)\n",
    "        predictions, alphas, betas, encoded_captions, decode_lengths, sort_idx = decoder(encoded_image, global_features, caps, caplens)\n",
    "        \n",
    "        # Since we decoded starting caption with <start> token, the targets are all words after <start> up to <end>\n",
    "        targets = encoded_captions[:, 1:]\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores, _ = pack_padded_sequence(predictions, decode_lengths, batch_first=True)\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Back prop\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "        \n",
    "        # Keep track if metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        \n",
    "        # Print status every print_freq\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                            loss=losses,\n",
    "                                                                            top5=top5accs))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, encoder, decoder, beam_size, epoch, vocab_size):\n",
    "    \"\"\"\n",
    "    Funtion to validate over the complete dataset\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    results = []\n",
    "\n",
    "    for i, (img, image_id) in enumerate(tqdm(val_loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
    "\n",
    "        k = beam_size\n",
    "        infinite_pred = False\n",
    "        \n",
    "        # Encode\n",
    "        image = img.to(device)       # (1, 3, 224, 224)\n",
    "        enc_image, global_features = encoder(image) # enc_image of shape (1,num_pixels,features)\n",
    "        # Flatten encoding\n",
    "        num_pixels = enc_image.size(1)\n",
    "        encoder_dim = enc_image.size(2)\n",
    "        # We'll treat the problem as having a batch size of k\n",
    "        enc_image = enc_image.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "        # Lists to store completed sequences, their alphas and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = decoder.init_hidden_state(enc_image)\n",
    "        spatial_image = F.relu(decoder.encoded_to_hidden(enc_image))  # (k,num_pixels,hidden_size)\n",
    "        global_image = F.relu(decoder.global_features(global_features))      # (1,embed_dim)\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (k,embed_dim)\n",
    "            inputs = torch.cat((embeddings, global_image.expand_as(embeddings)), dim = 1)    \n",
    "            h, c, st = decoder.LSTM(inputs , (h, c))  # (batch_size_t, hidden_size)\n",
    "            # Run the adaptive attention model\n",
    "            out_l, _, _ = decoder.adaptive_attention(spatial_image, h, st)\n",
    "            # Compute the probability over the vocabulary\n",
    "            scores = decoder.fc(out_l)      # (batch_size, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)   # (s, vocab_size)\n",
    "            # (k,1) will be (k,vocab_size), then (k,vocab_size) + (s,vocab_size) --> (s, vocab_size)\n",
    "            scores = top_k_scores.expand_as(scores) + scores  \n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                #Remember: torch.topk returns the top k scores in the first argument, and their respective indices in the second argument\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = top_k_words / vocab_size  # (s) \n",
    "            next_word_inds = top_k_words % vocab_size  # (s) \n",
    "            # Add new words to sequences, alphas\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            if k == 0:\n",
    "                break\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            seqs = seqs[incomplete_inds]              \n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            spatial_image = spatial_image[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                infinite_pred = True\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "            \n",
    "        if infinite_pred is not True:\n",
    "            i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "            seq = complete_seqs[i]\n",
    "        else:\n",
    "            seq = seqs[0][:20]\n",
    "            seq = [seq[i].item() for i in range(len(seq))]\n",
    "                \n",
    "        # Construct Sentence\n",
    "        sen_idx = [w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]\n",
    "        sentence = ' '.join([rev_word_map[sen_idx[i]] for i in range(len(sen_idx))])\n",
    "        item_dict = {\"image_id\": image_id.item(), \"caption\": sentence}\n",
    "        results.append(item_dict)\n",
    "    \n",
    "    print(\"Calculating Evalaution Metric Scores......\\n\")\n",
    "    \n",
    "    resFile = 'cococaptioncider/results/captions_val2014_results_' + str(epoch) + '.json' \n",
    "    evalFile = 'cococaptioncider/results/captions_val2014_eval_' + str(epoch) + '.json' \n",
    "    # Calculate Evaluation Scores\n",
    "    with open(resFile, 'w') as wr:\n",
    "        json.dump(results,wr)\n",
    "        \n",
    "    coco = COCO(annFile)\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "    # evaluate on a subset of images\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # evaluate results\n",
    "    cocoEval.evaluate()    \n",
    "    # Save Scores for all images in resFile\n",
    "    with open(evalFile, 'w') as w:\n",
    "        json.dump(cocoEval.eval, w)\n",
    "\n",
    "    return cocoEval.eval['CIDEr'], cocoEval.eval['Bleu_4']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('caption data/WORDMAP.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint is None:\n",
    "    decoder = Decoder(hidden_size, \n",
    "                      vocab_size=len(word_map), \n",
    "                      attention_dimension = attention_dim, \n",
    "                      embed_size = embed_dim,\n",
    "                      encoded_dimension = 2048) \n",
    "    \n",
    "    encoder = Encoder()\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr, betas = (0.8,0.999))\n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                         lr=encoder_lr, betas = (0.9,0.999)) if fine_tune_encoder else None\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "    best_cider = checkpoint['cider']\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "    if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "        encoder.fine_tune(fine_tune_encoder)\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),lr=encoder_lr)\n",
    "        print(\"Finetuning the CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU, if available\n",
    "decoder = decoder.to(device)\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(COCOTrainDataset(transform=transforms.Compose([normalize])),\n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(COCOValidationDataset(transform=transforms.Compose([normalize])),\n",
    "                                         batch_size = 1,\n",
    "                                         shuffle=True, \n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [4900 x 512], m2: [2048 x 512] at c:\\a\\w\\1\\s\\tmp_conda_3.7_110509\\conda\\conda-bld\\pytorch_1544094576194\\work\\aten\\src\\thc\\generic/THCTensorMathBlas.cu:266",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c19909ebde61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m          \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m          \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m          vocab_size=len(word_map))\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-4ad78c7005cc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch, vocab_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# Feed Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mencoded_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_captions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaplens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Since we decoded starting caption with <start> token, the targets are all words after <start> up to <end>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-a8837f098065>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, encoded_image, global_features, encoded_captions, caption_lengths)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mnum_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# get the spatial image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mspatial_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoded_to_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (batch_size, num_pixels, hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mglobal_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (batch_size, embed_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# sort input data by decreasing length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1354\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [4900 x 512], m2: [2048 x 512] at c:\\a\\w\\1\\s\\tmp_conda_3.7_110509\\conda\\conda-bld\\pytorch_1544094576194\\work\\aten\\src\\thc\\generic/THCTensorMathBlas.cu:266"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, 10):\n",
    "    if epochs_since_improvement == 8:\n",
    "        print('No Improvement for the last 8 epochs. Training terminated')\n",
    "        break\n",
    "    \n",
    "    # Decay\n",
    "    if epoch % 3 == 0 and epoch != 0:\n",
    "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "    \n",
    "    # One epoch training\n",
    "    train(train_loader=train_loader,\n",
    "         encoder=encoder,\n",
    "         decoder=decoder,\n",
    "         criterion=criterion,\n",
    "         encoder_optimizer=encoder_optimizer,\n",
    "         decoder_optimizer=decoder_optimizer,\n",
    "         epoch=epoch,\n",
    "         vocab_size=len(word_map))\n",
    "    \n",
    "    \n",
    "    recent_cider, recent_blue4 = validate(val_loader,\n",
    "                                         encoder,\n",
    "                                         decoder,\n",
    "                                         3,\n",
    "                                         epoch,\n",
    "                                         len(word_map))\n",
    "    print(\"Epoch {}:\\tCIDEr Score: {}\".format(epoch, recent_cider))\n",
    "    \n",
    "    # Check if there was an improvement\n",
    "    #recent_cider = best_cider + 0.01\n",
    "    is_best = recent_cider > best_cider\n",
    "    best_cider = max(recent_cider, best_cider)\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "        \n",
    "    print('Predict: ')\n",
    "    predict_output(\"test1.jpg\", rev_word_map)\n",
    "    \n",
    "\n",
    "    save_checkpoint(epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer, recent_cider, is_best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
