{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from scipy.misc import imread, imresize\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "#         resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        vgg = torchvision.models.vgg16(pretrained=True)\n",
    "        all_modules = list(vgg.children())\n",
    "        modules = all_modules[:-2]\n",
    "        self.vgg = nn.Sequential(*modules)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fine_tune()\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward function\n",
    "        input: - images : with shape (batch_size, 3, 224, 224) channel first\n",
    "        \"\"\"\n",
    "        # get the images features\n",
    "        encoded_image = self.vgg(images) # (batch_size, 512, 8, 8)\n",
    "        \n",
    "        batch_size = encoded_image.shape[0]\n",
    "        features = encoded_image.shape[1]\n",
    "        num_pixels = encoded_image.shape[2] * encoded_image.shape[3]\n",
    "        # get the global feature by using average pooling and rsshape it to batch_size, 512 (14)\n",
    "        global_features = self.avgpool(encoded_image).view(batch_size, -1) # (batch_size, 512)\n",
    "        # get the encoded image by resize the image feature\n",
    "        enc_image = encoded_image.permute(0, 2, 3, 1) # (batch_size, 7, ,7, 512)\n",
    "        enc_image = enc_image.view(batch_size, num_pixels, features) # (batch_size, num_pixels, 512)\n",
    "        return enc_image, global_features\n",
    "    \n",
    "    def fine_tune(self, status=False):\n",
    "        if not status:\n",
    "            for param in self.vgg.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for module in list(self.vgg.children())[5:]: # last layer only, len total layer is 8\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AdaptiveLSTMCell, self).__init__()\n",
    "        # create LSTM cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        # create input gate\n",
    "        self.x_gate = nn.Linear(input_size, hidden_size)\n",
    "        # crate hidden gate\n",
    "        self.h_gate = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, states):\n",
    "        \"\"\"\n",
    "        Forward function for LSTM\n",
    "        input: - x : word token combined with encoded_image\n",
    "               - states : the old hidden cell and memory cell\n",
    "        \"\"\"\n",
    "        h_old, m_old = states\n",
    "        # do LSTM, and get new hidden and output\n",
    "        ht, mt = self.lstm_cell(x, (h_old, m_old))\n",
    "        # do sigmoid to the input and hidden to get visual sentinel St (9)\n",
    "        gt = F.sigmoid(self.x_gate(x) + self.h_gate(h_old))\n",
    "        # and then do tanh to get visual sentinel (10)\n",
    "        st = gt * F.tanh(mt)\n",
    "        return ht, mt, st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_dimension):\n",
    "        super(AdaptiveAttention, self).__init__()\n",
    "        self.sentinel_affine= nn.Linear(hidden_size, hidden_size)\n",
    "        self.sentinel_attention = nn.Linear(hidden_size, attention_dimension)\n",
    "        self.hidden_affine = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden_attention = nn.Linear(hidden_size, attention_dimension)\n",
    "        self.visual_attention = nn.Linear(hidden_size, attention_dimension)\n",
    "        self.alphas = nn.Linear(attention_dimension, 1)\n",
    "        self.context_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "   \n",
    "    def forward(self, spatial_image, decoder_output, st):\n",
    "        \"\"\"\n",
    "        Forward function for Adaptive Attention\n",
    "        input: - spatial_image : the spatial image with shape (batch_size, num_pixels, hidden_size)\n",
    "               - decoder_output : the decoder hidden state with shape (batch_size, hidden_size)\n",
    "               - st : Visual sentinel returned by sentinel class with shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # extract num_pixels\n",
    "        num_pixels = spatial_image.shape[1]\n",
    "        # get the visual attention using spatial_image as input\n",
    "        visual_attn = self.visual_attention(spatial_image) # (batch_size, num_pixels, att_dim)\n",
    "        # get sentinel affine using st as input with ReLU activation\n",
    "        sentinel_affine = F.relu(self.sentinel_affine(st)) # (batch_size, hidden_size)\n",
    "        # get sentinel attention using sentinel_affine as input\n",
    "        sentinel_attn = self.sentinel_attention(sentinel_affine) # (batch_size, att_dim)\n",
    "        \n",
    "        hidden_affine = F.tanh(self.hidden_affine(decoder_output)) # (batch_sizem hidden_size)\n",
    "        hidden_attn = self.hidden_attention(hidden_affine) # (batch_size, attention_dimension)\n",
    "        \n",
    "        hidden_resized = hidden_attn.unsqueeze(1).expand(hidden_attn.size(0), num_pixels + 1, hidden_attn.size(1))\n",
    "        \n",
    "        concat_features = torch.cat([spatial_image, sentinel_affine.unsqueeze(1)], dim=1) # (batch_size, num_pixels+1, hidden_size)\n",
    "        attended_features = torch.cat([visual_attn, sentinel_attn.unsqueeze(1)], dim=1) # (batch_size, num_pixels, attn_dim)\n",
    "        \n",
    "        # do tanh to attended and hidden (6)\n",
    "        attention = F.tanh(attended_features + hidden_resized) # (batch_size, num_pixles+1, attn_dim)\n",
    "        # do a forward linear layer\n",
    "        alpha = self.alphas(attention).squeeze(2) # (batch_size, num_pixels+1)\n",
    "        # and do softmax\n",
    "        att_weights = F.softmax(alpha, dim=1) # (batch_size, num_pixels+1)\n",
    "        \n",
    "        context = (concat_features * att_weights.unsqueeze(2)).sum(dim=1) # (batch_size, hidden_size)\n",
    "        # get the new beta value by getting the last value of att_weights\n",
    "        beta_value = att_weights[:, -1].unsqueeze(1) #(batch_size, 1)\n",
    "        \n",
    "        out_l = F.tanh(self.context_hidden(context + hidden_affine))\n",
    "        \n",
    "        return out_l, att_weights, beta_value\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, attention_dimension, embed_size, encoded_dimension):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.encoded_to_hidden = nn.Linear(encoded_dimension, hidden_size)\n",
    "        self.global_features = nn.Linear(encoded_dimension, embed_size)\n",
    "        # input of the LSTMCell should be of shape (batch_size, input_size)\n",
    "        # because the input and global features are concenated, then input_features should be embed_size*2\n",
    "        self.LSTM = AdaptiveLSTMCell(embed_size*2, hidden_size)\n",
    "        self.adaptive_attention = AdaptiveAttention(hidden_size, attention_dimension)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def init_hidden_state(self, encoded_image):\n",
    "        h = torch.zeros(encoded_image.shape[0], 512).to(device)\n",
    "        c = torch.zeros(encoded_image.shape[0], 512).to(device)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoded_image, global_features, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward function for decoder\n",
    "        input: - encoded_image : the encoded images from encoder with shape (batch_size, num_pixels, 2048)\n",
    "               - global_features : the global features from encoder with shape (batch_size, 2048)\n",
    "               - encoded_captions : encoded captions with shape (batch_size, max_caption_length)\n",
    "               - caption_lengths : encoded caption length with dimension (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # extract the batch size and num_pixels\n",
    "        batch_size = encoded_image.shape[0]\n",
    "        num_pixels = encoded_image.shape[1]\n",
    "        # get the spatial image\n",
    "        spatial_image = F.relu(self.encoded_to_hidden(encoded_image)) # (batch_size, num_pixels, hidden_size)\n",
    "        global_image = F.relu(self.global_features(global_features)) # (batch_size, embed_size)\n",
    "        # sort input data by decreasing length\n",
    "        # caption_length will contains the sorted length, and sort_idx will contains the sorted elements indices\n",
    "        caption_lengths, sort_idx = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        # sort spatial_image, global_image, encoded_captions and encoded_image batches by caption length\n",
    "        spatial_image = spatial_image[sort_idx]\n",
    "        global_image = global_image[sort_idx]\n",
    "        encoded_captions = encoded_captions[sort_idx]\n",
    "        encoded_image = encoded_image[sort_idx]\n",
    "        \n",
    "        # Embedding, each batch contains a caption. All batch have the same number of rows (words), since we previously\n",
    "        # padded the ones shorter than max_caption_lengths, as well as the same number of columns (embed_dimension)\n",
    "        embeddings = self.embedding(encoded_captions) # (batch_size, max_caption_length, embed_dimesion)\n",
    "        \n",
    "        # initialize LSTM\n",
    "        h, c = self.init_hidden_state(encoded_image) # (batch_size, hidden_size)\n",
    "        \n",
    "        # we won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        decode_lengths =(caption_lengths - 1).tolist()\n",
    "        \n",
    "        # create tensors to store word prediction score, alphas and betas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels+1).to(device)\n",
    "        betas = torch.zeros(batch_size, max(decode_lengths), 1).to(device)\n",
    "        \n",
    "        # concenate the embeddings and global image feature for LSTM input\n",
    "        global_image = global_image.unsqueeze(1).expand_as(embeddings)\n",
    "        inputs = torch.cat((embeddings, global_image), dim=2) # (batch_size, max_caption_length, embed_dimension * 2)\n",
    "        \n",
    "        # start decoding\n",
    "        for timestep in range(max(decode_lengths)):\n",
    "            # create a packet sequence to process the only effective batch size N_t at that timestep\n",
    "            batch_size_t = sum([l > timestep for l in decode_lengths])\n",
    "            current_input = inputs[:batch_size_t, timestep, :] # (batch_size_t, embed_dimension * 2)\n",
    "            # do LSTM\n",
    "            h, c, st = self.LSTM(current_input, (h[:batch_size_t], c[:batch_size_t])) # (batch_size, hidden_size)\n",
    "            # run the adaptive attention\n",
    "            out_l, alpha_t, beta_t = self.adaptive_attention(spatial_image[:batch_size_t], h, st)\n",
    "            # compute the probability over the vocabulary with fullt connected layer\n",
    "            pred = self.fc(self.dropout(out_l))\n",
    "            # store the prediction, alphas and betas value\n",
    "            predictions[:batch_size_t, timestep, :] = pred\n",
    "            alphas[:batch_size_t, timestep, :] = alpha_t\n",
    "            betas[:batch_size_t, timestep, :] = beta_t\n",
    "        return predictions, alphas, betas, encoded_captions, decode_lengths, sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from cococaptioncider.pycocotools.coco import COCO\n",
    "from cococaptioncider.pycocoevalcap.eval import COCOEvalCap\n",
    "from util import *\n",
    "from dataset import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameter\n",
    "embed_dim = 512 # dimension of word embeddings\n",
    "attention_dim = 512 # attention hidden size\n",
    "hidden_size = 512 # dimension of decoder LSTM\n",
    "cudnn.benchmark = True # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Training parameter\n",
    "start_epoch = 0\n",
    "epochs = 20\n",
    "epochs_since_improvement = 0\n",
    "batch_size = 100\n",
    "workers = 1\n",
    "encoder_lr = 1e-4\n",
    "decoder_lr = 5e-4\n",
    "grad_clip = 0.1\n",
    "best_cider = 0\n",
    "print_freq = 100\n",
    "fine_tune_encoder = False\n",
    "checkpoint = None\n",
    "annFile = 'cococaptioncider/annotations/new_indo_caption_val.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch, vocab_size):\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "    losses = AverageMeter()\n",
    "    top5accs = AverageMeter()\n",
    "    \n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "        # move to GPU\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "        \n",
    "        # Feed Forward\n",
    "        encoded_image, global_features = encoder(imgs)\n",
    "        predictions, alphas, betas, encoded_captions, decode_lengths, sort_idx = decoder(encoded_image, global_features, caps, caplens)\n",
    "        \n",
    "        # Since we decoded starting caption with <start> token, the targets are all words after <start> up to <end>\n",
    "        targets = encoded_captions[:, 1:]\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores, _ = pack_padded_sequence(predictions, decode_lengths, batch_first=True)\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Back prop\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "        \n",
    "        # Keep track if metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        \n",
    "        # Print status every print_freq\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                            loss=losses,\n",
    "                                                                            top5=top5accs))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, encoder, decoder, beam_size, epoch, vocab_size):\n",
    "    \"\"\"\n",
    "    Funtion to validate over the complete dataset\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    results = []\n",
    "\n",
    "    for i, (img, image_id) in enumerate(tqdm(val_loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
    "\n",
    "        k = beam_size\n",
    "        infinite_pred = False\n",
    "        \n",
    "        # Encode\n",
    "        image = img.to(device)       # (1, 3, 224, 224)\n",
    "        enc_image, global_features = encoder(image) # enc_image of shape (1,num_pixels,features)\n",
    "        # Flatten encoding\n",
    "        num_pixels = enc_image.size(1)\n",
    "        encoder_dim = enc_image.size(2)\n",
    "        # We'll treat the problem as having a batch size of k\n",
    "        enc_image = enc_image.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "        # Lists to store completed sequences, their alphas and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = decoder.init_hidden_state(enc_image)\n",
    "        spatial_image = F.relu(decoder.encoded_to_hidden(enc_image))  # (k,num_pixels,hidden_size)\n",
    "        global_image = F.relu(decoder.global_features(global_features))      # (1,embed_dim)\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (k,embed_dim)\n",
    "            inputs = torch.cat((embeddings, global_image.expand_as(embeddings)), dim = 1)    \n",
    "            h, c, st = decoder.LSTM(inputs , (h, c))  # (batch_size_t, hidden_size)\n",
    "            # Run the adaptive attention model\n",
    "            out_l, _, _ = decoder.adaptive_attention(spatial_image, h, st)\n",
    "            # Compute the probability over the vocabulary\n",
    "            scores = decoder.fc(out_l)      # (batch_size, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)   # (s, vocab_size)\n",
    "            # (k,1) will be (k,vocab_size), then (k,vocab_size) + (s,vocab_size) --> (s, vocab_size)\n",
    "            scores = top_k_scores.expand_as(scores) + scores  \n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                #Remember: torch.topk returns the top k scores in the first argument, and their respective indices in the second argument\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = top_k_words / vocab_size  # (s) \n",
    "            next_word_inds = top_k_words % vocab_size  # (s) \n",
    "            # Add new words to sequences, alphas\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            if k == 0:\n",
    "                break\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            seqs = seqs[incomplete_inds]              \n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            spatial_image = spatial_image[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                infinite_pred = True\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "            \n",
    "        if infinite_pred is not True:\n",
    "            i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "            seq = complete_seqs[i]\n",
    "        else:\n",
    "            seq = seqs[0][:20]\n",
    "            seq = [seq[i].item() for i in range(len(seq))]\n",
    "                \n",
    "        # Construct Sentence\n",
    "        sen_idx = [w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]\n",
    "        sentence = ' '.join([rev_word_map[sen_idx[i]] for i in range(len(sen_idx))])\n",
    "        item_dict = {\"image_id\": image_id.item(), \"caption\": sentence}\n",
    "        results.append(item_dict)\n",
    "    \n",
    "    print(\"Calculating Evalaution Metric Scores......\\n\")\n",
    "    \n",
    "    resFile = 'cococaptioncider/results/captions_val2014_results_' + str(epoch) + '.json' \n",
    "    evalFile = 'cococaptioncider/results/captions_val2014_eval_' + str(epoch) + '.json' \n",
    "    # Calculate Evaluation Scores\n",
    "    with open(resFile, 'w') as wr:\n",
    "        json.dump(results,wr)\n",
    "        \n",
    "    coco = COCO(annFile)\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "    # evaluate on a subset of images\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # evaluate results\n",
    "    cocoEval.evaluate()    \n",
    "    # Save Scores for all images in resFile\n",
    "    with open(evalFile, 'w') as w:\n",
    "        json.dump(cocoEval.eval, w)\n",
    "\n",
    "    return cocoEval.eval['CIDEr'], cocoEval.eval['Bleu_4']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('caption data/WORDMAP.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint is None:\n",
    "    decoder = Decoder(hidden_size, \n",
    "                      vocab_size=len(word_map), \n",
    "                      attention_dimension = attention_dim, \n",
    "                      embed_size = embed_dim,\n",
    "                      encoded_dimension = 512) \n",
    "    \n",
    "    encoder = Encoder()\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr, betas = (0.8,0.999))\n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                         lr=encoder_lr, betas = (0.9,0.999)) if fine_tune_encoder else None\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "    best_cider = checkpoint['cider']\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "    if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "        encoder.fine_tune(fine_tune_encoder)\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),lr=encoder_lr)\n",
    "        print(\"Finetuning the CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU, if available\n",
    "decoder = decoder.to(device)\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = \"C:/Users/share/Documents/Machine Learning/Datasets/karpathy/output/TRAIN_IMAGES_coco_5_cap_per_img_100_min_word_freq.hdf5\"\n",
    "#caption = \"C:/Users/share/Documents/Machine Learning/Datasets/karpathy/output/TRAIN_CAPTIONS_coco_5_cap_per_img_100_min_word_freq.json\"\n",
    "#caplens = \"C:/Users/share/Documents/Machine Learning/Datasets/karpathy/output/TRAIN_CAPLENS_coco_5_cap_per_img_100_min_word_freq.json\"\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(COCOTrainDataset(transform=transforms.Compose([normalize])),\n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)\n",
    "\n",
    "#validation_path = \"C:/Users/share/Documents/Machine Learning/Datasets/karpathy/output/TRAIN_IMAGES_coco_5_cap_per_img_100_min_word_freq.hdf5\"\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(COCOValidationDataset(transform=transforms.Compose([normalize])),\n",
    "                                         batch_size = 1,\n",
    "                                         shuffle=True, \n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/5729]\tLoss 2.9255 (2.9255)\tTop-5 Accuracy 65.765 (65.765)\t\n",
      "Epoch: [1][100/5729]\tLoss 2.9783 (3.0864)\tTop-5 Accuracy 66.283 (64.068)\t\n",
      "Epoch: [1][200/5729]\tLoss 3.1902 (3.0779)\tTop-5 Accuracy 62.959 (64.323)\t\n",
      "Epoch: [1][300/5729]\tLoss 3.1377 (3.0798)\tTop-5 Accuracy 63.796 (64.319)\t\n",
      "Epoch: [1][400/5729]\tLoss 3.1814 (3.0823)\tTop-5 Accuracy 61.426 (64.327)\t\n",
      "Epoch: [1][500/5729]\tLoss 3.1644 (3.0826)\tTop-5 Accuracy 61.516 (64.307)\t\n",
      "Epoch: [1][600/5729]\tLoss 3.1464 (3.0816)\tTop-5 Accuracy 64.734 (64.280)\t\n",
      "Epoch: [1][700/5729]\tLoss 3.0619 (3.0783)\tTop-5 Accuracy 64.458 (64.346)\t\n",
      "Epoch: [1][800/5729]\tLoss 3.1007 (3.0761)\tTop-5 Accuracy 64.875 (64.384)\t\n",
      "Epoch: [1][900/5729]\tLoss 3.1417 (3.0742)\tTop-5 Accuracy 62.464 (64.389)\t\n",
      "Epoch: [1][1000/5729]\tLoss 3.0270 (3.0741)\tTop-5 Accuracy 67.554 (64.397)\t\n",
      "Epoch: [1][1100/5729]\tLoss 2.9033 (3.0742)\tTop-5 Accuracy 66.114 (64.401)\t\n",
      "Epoch: [1][1200/5729]\tLoss 2.9569 (3.0724)\tTop-5 Accuracy 65.610 (64.432)\t\n",
      "Epoch: [1][1300/5729]\tLoss 3.1001 (3.0708)\tTop-5 Accuracy 62.838 (64.455)\t\n",
      "Epoch: [1][1400/5729]\tLoss 2.9850 (3.0680)\tTop-5 Accuracy 65.676 (64.503)\t\n",
      "Epoch: [1][1500/5729]\tLoss 3.0054 (3.0682)\tTop-5 Accuracy 66.041 (64.511)\t\n",
      "Epoch: [1][1600/5729]\tLoss 3.0082 (3.0670)\tTop-5 Accuracy 64.229 (64.527)\t\n",
      "Epoch: [1][1700/5729]\tLoss 3.0134 (3.0665)\tTop-5 Accuracy 67.552 (64.529)\t\n",
      "Epoch: [1][1800/5729]\tLoss 3.2203 (3.0649)\tTop-5 Accuracy 63.209 (64.550)\t\n",
      "Epoch: [1][1900/5729]\tLoss 2.9548 (3.0643)\tTop-5 Accuracy 66.300 (64.565)\t\n",
      "Epoch: [1][2000/5729]\tLoss 3.0971 (3.0628)\tTop-5 Accuracy 64.683 (64.585)\t\n",
      "Epoch: [1][2100/5729]\tLoss 3.1255 (3.0622)\tTop-5 Accuracy 63.559 (64.591)\t\n",
      "Epoch: [1][2200/5729]\tLoss 3.0384 (3.0611)\tTop-5 Accuracy 65.419 (64.597)\t\n",
      "Epoch: [1][2300/5729]\tLoss 2.9342 (3.0603)\tTop-5 Accuracy 67.287 (64.604)\t\n",
      "Epoch: [1][2400/5729]\tLoss 3.1333 (3.0599)\tTop-5 Accuracy 64.172 (64.611)\t\n",
      "Epoch: [1][2500/5729]\tLoss 3.1873 (3.0584)\tTop-5 Accuracy 63.939 (64.632)\t\n",
      "Epoch: [1][2600/5729]\tLoss 3.1165 (3.0583)\tTop-5 Accuracy 63.436 (64.639)\t\n",
      "Epoch: [1][2700/5729]\tLoss 3.0146 (3.0572)\tTop-5 Accuracy 67.389 (64.662)\t\n",
      "Epoch: [1][2800/5729]\tLoss 2.9004 (3.0561)\tTop-5 Accuracy 67.717 (64.674)\t\n",
      "Epoch: [1][2900/5729]\tLoss 3.0882 (3.0551)\tTop-5 Accuracy 65.248 (64.694)\t\n",
      "Epoch: [1][3000/5729]\tLoss 2.8885 (3.0541)\tTop-5 Accuracy 67.385 (64.711)\t\n",
      "Epoch: [1][3100/5729]\tLoss 3.0032 (3.0526)\tTop-5 Accuracy 63.746 (64.730)\t\n",
      "Epoch: [1][3200/5729]\tLoss 2.8520 (3.0521)\tTop-5 Accuracy 66.203 (64.744)\t\n",
      "Epoch: [1][3300/5729]\tLoss 2.9036 (3.0512)\tTop-5 Accuracy 67.471 (64.758)\t\n",
      "Epoch: [1][3400/5729]\tLoss 2.9473 (3.0497)\tTop-5 Accuracy 64.898 (64.779)\t\n",
      "Epoch: [1][3500/5729]\tLoss 2.9842 (3.0487)\tTop-5 Accuracy 68.223 (64.797)\t\n",
      "Epoch: [1][3600/5729]\tLoss 3.1489 (3.0479)\tTop-5 Accuracy 64.796 (64.812)\t\n",
      "Epoch: [1][3700/5729]\tLoss 3.0484 (3.0474)\tTop-5 Accuracy 66.397 (64.827)\t\n",
      "Epoch: [1][3800/5729]\tLoss 3.0367 (3.0465)\tTop-5 Accuracy 65.832 (64.842)\t\n",
      "Epoch: [1][3900/5729]\tLoss 2.8647 (3.0451)\tTop-5 Accuracy 66.065 (64.862)\t\n",
      "Epoch: [1][4000/5729]\tLoss 3.0648 (3.0440)\tTop-5 Accuracy 63.836 (64.882)\t\n",
      "Epoch: [1][4100/5729]\tLoss 3.0919 (3.0427)\tTop-5 Accuracy 63.736 (64.906)\t\n",
      "Epoch: [1][4200/5729]\tLoss 2.9156 (3.0422)\tTop-5 Accuracy 67.003 (64.913)\t\n",
      "Epoch: [1][4300/5729]\tLoss 2.8507 (3.0414)\tTop-5 Accuracy 68.273 (64.924)\t\n",
      "Epoch: [1][4400/5729]\tLoss 2.8408 (3.0406)\tTop-5 Accuracy 67.732 (64.938)\t\n",
      "Epoch: [1][4500/5729]\tLoss 2.8540 (3.0400)\tTop-5 Accuracy 67.181 (64.945)\t\n",
      "Epoch: [1][4600/5729]\tLoss 2.7945 (3.0387)\tTop-5 Accuracy 68.124 (64.963)\t\n",
      "Epoch: [1][4700/5729]\tLoss 2.9784 (3.0380)\tTop-5 Accuracy 66.826 (64.975)\t\n",
      "Epoch: [1][4800/5729]\tLoss 2.9528 (3.0369)\tTop-5 Accuracy 67.053 (64.989)\t\n",
      "Epoch: [1][4900/5729]\tLoss 3.1641 (3.0357)\tTop-5 Accuracy 62.813 (65.006)\t\n",
      "Epoch: [1][5000/5729]\tLoss 2.9857 (3.0346)\tTop-5 Accuracy 66.667 (65.024)\t\n",
      "Epoch: [1][5100/5729]\tLoss 2.9096 (3.0337)\tTop-5 Accuracy 65.514 (65.036)\t\n",
      "Epoch: [1][5200/5729]\tLoss 2.7814 (3.0330)\tTop-5 Accuracy 69.038 (65.042)\t\n",
      "Epoch: [1][5300/5729]\tLoss 2.9327 (3.0325)\tTop-5 Accuracy 67.060 (65.050)\t\n",
      "Epoch: [1][5400/5729]\tLoss 2.6305 (3.0315)\tTop-5 Accuracy 71.111 (65.065)\t\n",
      "Epoch: [1][5500/5729]\tLoss 3.0162 (3.0307)\tTop-5 Accuracy 64.495 (65.078)\t\n",
      "Epoch: [1][5600/5729]\tLoss 3.0044 (3.0298)\tTop-5 Accuracy 64.688 (65.091)\t\n",
      "Epoch: [1][5700/5729]\tLoss 3.1722 (3.0289)\tTop-5 Accuracy 62.130 (65.104)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [47:02<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.543992\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 300885, 'reflen': 306886, 'guess': [300885, 260381, 219877, 179373], 'correct': [197924, 96421, 40966, 17056]}\n",
      "ratio: 0.9804455074522755\n",
      "Bleu_1: 0.645\n",
      "Bleu_2: 0.484\n",
      "Bleu_3: 0.350\n",
      "Bleu_4: 0.251\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.889\n",
      "Epoch 1:\tCIDEr Score: 0.8887264183756494\n",
      "Predict: \n",
      "sebuah sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n",
      "Epoch: [2][0/5729]\tLoss 2.8062 (2.8062)\tTop-5 Accuracy 68.781 (68.781)\t\n",
      "Epoch: [2][100/5729]\tLoss 2.6551 (2.8564)\tTop-5 Accuracy 69.776 (67.404)\t\n",
      "Epoch: [2][200/5729]\tLoss 2.8643 (2.8703)\tTop-5 Accuracy 66.433 (67.203)\t\n",
      "Epoch: [2][300/5729]\tLoss 2.8204 (2.8773)\tTop-5 Accuracy 69.087 (67.068)\t\n",
      "Epoch: [2][400/5729]\tLoss 2.8934 (2.8824)\tTop-5 Accuracy 68.750 (67.002)\t\n",
      "Epoch: [2][500/5729]\tLoss 2.9121 (2.8809)\tTop-5 Accuracy 66.342 (67.020)\t\n",
      "Epoch: [2][600/5729]\tLoss 2.8699 (2.8820)\tTop-5 Accuracy 66.573 (66.990)\t\n",
      "Epoch: [2][700/5729]\tLoss 2.8723 (2.8829)\tTop-5 Accuracy 68.830 (66.992)\t\n",
      "Epoch: [2][800/5729]\tLoss 2.8348 (2.8851)\tTop-5 Accuracy 67.039 (66.953)\t\n",
      "Epoch: [2][900/5729]\tLoss 2.8538 (2.8884)\tTop-5 Accuracy 66.930 (66.923)\t\n",
      "Epoch: [2][1000/5729]\tLoss 3.0025 (2.8895)\tTop-5 Accuracy 65.061 (66.925)\t\n",
      "Epoch: [2][1100/5729]\tLoss 2.6595 (2.8900)\tTop-5 Accuracy 69.736 (66.921)\t\n",
      "Epoch: [2][1200/5729]\tLoss 2.7894 (2.8912)\tTop-5 Accuracy 68.660 (66.896)\t\n",
      "Epoch: [2][1300/5729]\tLoss 2.9379 (2.8915)\tTop-5 Accuracy 65.875 (66.904)\t\n",
      "Epoch: [2][1400/5729]\tLoss 2.7669 (2.8911)\tTop-5 Accuracy 70.102 (66.921)\t\n",
      "Epoch: [2][1500/5729]\tLoss 2.9413 (2.8910)\tTop-5 Accuracy 66.206 (66.916)\t\n",
      "Epoch: [2][1600/5729]\tLoss 3.0372 (2.8905)\tTop-5 Accuracy 63.923 (66.925)\t\n",
      "Epoch: [2][1700/5729]\tLoss 2.8032 (2.8907)\tTop-5 Accuracy 67.615 (66.927)\t\n",
      "Epoch: [2][1800/5729]\tLoss 2.8755 (2.8909)\tTop-5 Accuracy 67.085 (66.922)\t\n",
      "Epoch: [2][1900/5729]\tLoss 3.0418 (2.8906)\tTop-5 Accuracy 66.078 (66.936)\t\n",
      "Epoch: [2][2000/5729]\tLoss 2.8844 (2.8896)\tTop-5 Accuracy 67.813 (66.952)\t\n",
      "Epoch: [2][2100/5729]\tLoss 3.0107 (2.8894)\tTop-5 Accuracy 65.381 (66.961)\t\n",
      "Epoch: [2][2200/5729]\tLoss 2.9522 (2.8891)\tTop-5 Accuracy 67.215 (66.969)\t\n",
      "Epoch: [2][2300/5729]\tLoss 2.9920 (2.8891)\tTop-5 Accuracy 67.383 (66.969)\t\n",
      "Epoch: [2][2400/5729]\tLoss 2.9081 (2.8893)\tTop-5 Accuracy 67.192 (66.965)\t\n",
      "Epoch: [2][2500/5729]\tLoss 2.7932 (2.8885)\tTop-5 Accuracy 68.882 (66.979)\t\n",
      "Epoch: [2][2600/5729]\tLoss 3.0470 (2.8882)\tTop-5 Accuracy 66.378 (66.987)\t\n",
      "Epoch: [2][2700/5729]\tLoss 2.8220 (2.8885)\tTop-5 Accuracy 67.300 (66.979)\t\n",
      "Epoch: [2][2800/5729]\tLoss 2.7809 (2.8885)\tTop-5 Accuracy 67.901 (66.974)\t\n",
      "Epoch: [2][2900/5729]\tLoss 3.0122 (2.8886)\tTop-5 Accuracy 65.344 (66.967)\t\n",
      "Epoch: [2][3000/5729]\tLoss 2.7980 (2.8887)\tTop-5 Accuracy 69.208 (66.961)\t\n",
      "Epoch: [2][3100/5729]\tLoss 2.9620 (2.8885)\tTop-5 Accuracy 67.608 (66.962)\t\n",
      "Epoch: [2][3200/5729]\tLoss 2.9803 (2.8889)\tTop-5 Accuracy 64.742 (66.957)\t\n",
      "Epoch: [2][3300/5729]\tLoss 2.7928 (2.8895)\tTop-5 Accuracy 67.291 (66.947)\t\n",
      "Epoch: [2][3400/5729]\tLoss 2.9188 (2.8887)\tTop-5 Accuracy 68.110 (66.960)\t\n",
      "Epoch: [2][3500/5729]\tLoss 2.9253 (2.8885)\tTop-5 Accuracy 66.429 (66.965)\t\n",
      "Epoch: [2][3600/5729]\tLoss 2.9667 (2.8889)\tTop-5 Accuracy 64.794 (66.962)\t\n",
      "Epoch: [2][3700/5729]\tLoss 2.9094 (2.8888)\tTop-5 Accuracy 65.751 (66.964)\t\n",
      "Epoch: [2][3800/5729]\tLoss 2.7759 (2.8891)\tTop-5 Accuracy 67.433 (66.961)\t\n",
      "Epoch: [2][3900/5729]\tLoss 2.9202 (2.8889)\tTop-5 Accuracy 67.181 (66.964)\t\n",
      "Epoch: [2][4000/5729]\tLoss 3.0348 (2.8886)\tTop-5 Accuracy 66.532 (66.971)\t\n",
      "Epoch: [2][4100/5729]\tLoss 2.9760 (2.8886)\tTop-5 Accuracy 65.533 (66.969)\t\n",
      "Epoch: [2][4200/5729]\tLoss 2.8872 (2.8886)\tTop-5 Accuracy 67.155 (66.968)\t\n",
      "Epoch: [2][4300/5729]\tLoss 3.0059 (2.8889)\tTop-5 Accuracy 65.221 (66.965)\t\n",
      "Epoch: [2][4400/5729]\tLoss 2.7998 (2.8886)\tTop-5 Accuracy 67.402 (66.973)\t\n",
      "Epoch: [2][4500/5729]\tLoss 2.6779 (2.8889)\tTop-5 Accuracy 69.907 (66.974)\t\n",
      "Epoch: [2][4600/5729]\tLoss 2.7524 (2.8885)\tTop-5 Accuracy 68.627 (66.980)\t\n",
      "Epoch: [2][4700/5729]\tLoss 2.8047 (2.8886)\tTop-5 Accuracy 67.310 (66.979)\t\n",
      "Epoch: [2][4800/5729]\tLoss 2.6953 (2.8889)\tTop-5 Accuracy 69.582 (66.977)\t\n",
      "Epoch: [2][4900/5729]\tLoss 2.7717 (2.8888)\tTop-5 Accuracy 68.200 (66.978)\t\n",
      "Epoch: [2][5000/5729]\tLoss 2.8841 (2.8886)\tTop-5 Accuracy 66.698 (66.983)\t\n",
      "Epoch: [2][5100/5729]\tLoss 2.9135 (2.8884)\tTop-5 Accuracy 66.667 (66.985)\t\n",
      "Epoch: [2][5200/5729]\tLoss 2.9784 (2.8883)\tTop-5 Accuracy 65.196 (66.986)\t\n",
      "Epoch: [2][5300/5729]\tLoss 2.8316 (2.8880)\tTop-5 Accuracy 67.647 (66.992)\t\n",
      "Epoch: [2][5400/5729]\tLoss 2.8580 (2.8878)\tTop-5 Accuracy 67.086 (66.996)\t\n",
      "Epoch: [2][5500/5729]\tLoss 2.8436 (2.8876)\tTop-5 Accuracy 67.497 (66.998)\t\n",
      "Epoch: [2][5600/5729]\tLoss 2.9385 (2.8871)\tTop-5 Accuracy 68.624 (67.006)\t\n",
      "Epoch: [2][5700/5729]\tLoss 2.7139 (2.8869)\tTop-5 Accuracy 68.923 (67.011)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [47:30<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.542001\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 299223, 'reflen': 305641, 'guess': [299223, 258719, 218215, 177711], 'correct': [196999, 96408, 41605, 17324]}\n",
      "ratio: 0.9790015083054925\n",
      "Bleu_1: 0.644\n",
      "Bleu_2: 0.485\n",
      "Bleu_3: 0.353\n",
      "Bleu_4: 0.254\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.902\n",
      "Epoch 2:\tCIDEr Score: 0.9021348722085023\n",
      "Predict: \n",
      "sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n",
      "\n",
      "DECAYING learning rate.\n",
      "The new learning rate is 0.000400\n",
      "\n",
      "Epoch: [3][0/5729]\tLoss 2.8223 (2.8223)\tTop-5 Accuracy 67.638 (67.638)\t\n",
      "Epoch: [3][100/5729]\tLoss 2.8444 (2.7731)\tTop-5 Accuracy 67.026 (68.586)\t\n",
      "Epoch: [3][200/5729]\tLoss 2.7380 (2.7610)\tTop-5 Accuracy 68.519 (68.681)\t\n",
      "Epoch: [3][300/5729]\tLoss 2.5743 (2.7590)\tTop-5 Accuracy 71.095 (68.718)\t\n",
      "Epoch: [3][400/5729]\tLoss 2.6903 (2.7625)\tTop-5 Accuracy 69.497 (68.708)\t\n",
      "Epoch: [3][500/5729]\tLoss 2.7078 (2.7606)\tTop-5 Accuracy 69.536 (68.755)\t\n",
      "Epoch: [3][600/5729]\tLoss 2.7483 (2.7588)\tTop-5 Accuracy 69.513 (68.786)\t\n",
      "Epoch: [3][700/5729]\tLoss 2.6642 (2.7567)\tTop-5 Accuracy 69.949 (68.800)\t\n",
      "Epoch: [3][800/5729]\tLoss 2.8655 (2.7577)\tTop-5 Accuracy 66.829 (68.772)\t\n",
      "Epoch: [3][900/5729]\tLoss 2.7905 (2.7586)\tTop-5 Accuracy 68.939 (68.757)\t\n",
      "Epoch: [3][1000/5729]\tLoss 2.7463 (2.7593)\tTop-5 Accuracy 71.073 (68.754)\t\n",
      "Epoch: [3][1100/5729]\tLoss 2.8669 (2.7602)\tTop-5 Accuracy 67.211 (68.740)\t\n",
      "Epoch: [3][1200/5729]\tLoss 2.7002 (2.7625)\tTop-5 Accuracy 69.794 (68.704)\t\n",
      "Epoch: [3][1300/5729]\tLoss 2.7544 (2.7614)\tTop-5 Accuracy 67.719 (68.716)\t\n",
      "Epoch: [3][1400/5729]\tLoss 2.8596 (2.7621)\tTop-5 Accuracy 66.440 (68.709)\t\n",
      "Epoch: [3][1500/5729]\tLoss 2.8137 (2.7620)\tTop-5 Accuracy 68.780 (68.709)\t\n",
      "Epoch: [3][1600/5729]\tLoss 2.6192 (2.7620)\tTop-5 Accuracy 71.078 (68.710)\t\n",
      "Epoch: [3][1700/5729]\tLoss 2.5415 (2.7622)\tTop-5 Accuracy 71.741 (68.705)\t\n",
      "Epoch: [3][1800/5729]\tLoss 2.5507 (2.7621)\tTop-5 Accuracy 71.513 (68.711)\t\n",
      "Epoch: [3][1900/5729]\tLoss 2.7311 (2.7627)\tTop-5 Accuracy 68.936 (68.699)\t\n",
      "Epoch: [3][2000/5729]\tLoss 2.9280 (2.7633)\tTop-5 Accuracy 65.711 (68.687)\t\n",
      "Epoch: [3][2100/5729]\tLoss 2.8372 (2.7646)\tTop-5 Accuracy 68.015 (68.667)\t\n",
      "Epoch: [3][2200/5729]\tLoss 2.7166 (2.7641)\tTop-5 Accuracy 68.939 (68.677)\t\n",
      "Epoch: [3][2300/5729]\tLoss 2.8668 (2.7654)\tTop-5 Accuracy 68.021 (68.665)\t\n",
      "Epoch: [3][2400/5729]\tLoss 2.7836 (2.7652)\tTop-5 Accuracy 67.717 (68.669)\t\n",
      "Epoch: [3][2500/5729]\tLoss 2.7099 (2.7658)\tTop-5 Accuracy 69.524 (68.662)\t\n",
      "Epoch: [3][2600/5729]\tLoss 2.8645 (2.7666)\tTop-5 Accuracy 66.881 (68.655)\t\n",
      "Epoch: [3][2700/5729]\tLoss 2.9458 (2.7672)\tTop-5 Accuracy 66.822 (68.649)\t\n",
      "Epoch: [3][2800/5729]\tLoss 2.7456 (2.7676)\tTop-5 Accuracy 68.876 (68.641)\t\n",
      "Epoch: [3][2900/5729]\tLoss 2.7199 (2.7677)\tTop-5 Accuracy 69.917 (68.645)\t\n",
      "Epoch: [3][3000/5729]\tLoss 2.6928 (2.7675)\tTop-5 Accuracy 71.357 (68.652)\t\n",
      "Epoch: [3][3100/5729]\tLoss 2.7792 (2.7676)\tTop-5 Accuracy 69.497 (68.656)\t\n",
      "Epoch: [3][3200/5729]\tLoss 2.8022 (2.7678)\tTop-5 Accuracy 67.548 (68.653)\t\n",
      "Epoch: [3][3300/5729]\tLoss 2.8038 (2.7682)\tTop-5 Accuracy 68.443 (68.651)\t\n",
      "Epoch: [3][3400/5729]\tLoss 2.9179 (2.7685)\tTop-5 Accuracy 67.065 (68.647)\t\n",
      "Epoch: [3][3500/5729]\tLoss 2.8036 (2.7686)\tTop-5 Accuracy 67.361 (68.651)\t\n",
      "Epoch: [3][3600/5729]\tLoss 2.6240 (2.7686)\tTop-5 Accuracy 73.203 (68.652)\t\n",
      "Epoch: [3][3700/5729]\tLoss 2.8370 (2.7695)\tTop-5 Accuracy 67.797 (68.639)\t\n",
      "Epoch: [3][3800/5729]\tLoss 2.8053 (2.7697)\tTop-5 Accuracy 67.381 (68.634)\t\n",
      "Epoch: [3][3900/5729]\tLoss 2.8296 (2.7698)\tTop-5 Accuracy 66.734 (68.634)\t\n",
      "Epoch: [3][4000/5729]\tLoss 2.7341 (2.7699)\tTop-5 Accuracy 68.199 (68.635)\t\n",
      "Epoch: [3][4100/5729]\tLoss 2.7484 (2.7699)\tTop-5 Accuracy 67.761 (68.637)\t\n",
      "Epoch: [3][4200/5729]\tLoss 2.6586 (2.7698)\tTop-5 Accuracy 70.791 (68.639)\t\n",
      "Epoch: [3][4300/5729]\tLoss 2.7549 (2.7698)\tTop-5 Accuracy 68.756 (68.639)\t\n",
      "Epoch: [3][4400/5729]\tLoss 2.7999 (2.7697)\tTop-5 Accuracy 68.281 (68.639)\t\n",
      "Epoch: [3][4500/5729]\tLoss 2.8950 (2.7697)\tTop-5 Accuracy 67.169 (68.638)\t\n",
      "Epoch: [3][4600/5729]\tLoss 2.7464 (2.7697)\tTop-5 Accuracy 70.000 (68.639)\t\n",
      "Epoch: [3][4700/5729]\tLoss 2.7550 (2.7695)\tTop-5 Accuracy 68.809 (68.641)\t\n",
      "Epoch: [3][4800/5729]\tLoss 2.7229 (2.7695)\tTop-5 Accuracy 69.930 (68.643)\t\n",
      "Epoch: [3][4900/5729]\tLoss 2.8629 (2.7697)\tTop-5 Accuracy 68.421 (68.640)\t\n",
      "Epoch: [3][5000/5729]\tLoss 2.6629 (2.7695)\tTop-5 Accuracy 68.667 (68.643)\t\n",
      "Epoch: [3][5100/5729]\tLoss 2.8168 (2.7695)\tTop-5 Accuracy 67.729 (68.645)\t\n",
      "Epoch: [3][5200/5729]\tLoss 2.5988 (2.7698)\tTop-5 Accuracy 70.995 (68.645)\t\n",
      "Epoch: [3][5300/5729]\tLoss 2.8081 (2.7701)\tTop-5 Accuracy 68.016 (68.643)\t\n",
      "Epoch: [3][5400/5729]\tLoss 2.9687 (2.7699)\tTop-5 Accuracy 65.097 (68.647)\t\n",
      "Epoch: [3][5500/5729]\tLoss 2.6698 (2.7697)\tTop-5 Accuracy 69.910 (68.649)\t\n",
      "Epoch: [3][5600/5729]\tLoss 2.7355 (2.7696)\tTop-5 Accuracy 70.577 (68.654)\t\n",
      "Epoch: [3][5700/5729]\tLoss 2.6584 (2.7696)\tTop-5 Accuracy 69.632 (68.656)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [45:41<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.521995\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 298729, 'reflen': 305165, 'guess': [298729, 258225, 217721, 177217], 'correct': [198104, 96208, 41688, 17700]}\n",
      "ratio: 0.9789097701243558\n",
      "Bleu_1: 0.649\n",
      "Bleu_2: 0.486\n",
      "Bleu_3: 0.354\n",
      "Bleu_4: 0.257\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.915\n",
      "Epoch 3:\tCIDEr Score: 0.9148574687741924\n",
      "Predict: \n",
      "sebuah sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n",
      "Epoch: [4][0/5729]\tLoss 2.7087 (2.7087)\tTop-5 Accuracy 68.586 (68.586)\t\n",
      "Epoch: [4][100/5729]\tLoss 2.6717 (2.6759)\tTop-5 Accuracy 69.223 (69.858)\t\n",
      "Epoch: [4][200/5729]\tLoss 2.6700 (2.6731)\tTop-5 Accuracy 71.257 (69.934)\t\n",
      "Epoch: [4][300/5729]\tLoss 2.6694 (2.6752)\tTop-5 Accuracy 70.000 (69.936)\t\n",
      "Epoch: [4][400/5729]\tLoss 2.7085 (2.6731)\tTop-5 Accuracy 69.149 (69.974)\t\n",
      "Epoch: [4][500/5729]\tLoss 2.5112 (2.6791)\tTop-5 Accuracy 73.621 (69.890)\t\n",
      "Epoch: [4][600/5729]\tLoss 2.5783 (2.6794)\tTop-5 Accuracy 71.212 (69.867)\t\n",
      "Epoch: [4][700/5729]\tLoss 2.6579 (2.6780)\tTop-5 Accuracy 68.999 (69.893)\t\n",
      "Epoch: [4][800/5729]\tLoss 2.7825 (2.6786)\tTop-5 Accuracy 67.850 (69.900)\t\n",
      "Epoch: [4][900/5729]\tLoss 2.5285 (2.6787)\tTop-5 Accuracy 72.755 (69.894)\t\n",
      "Epoch: [4][1000/5729]\tLoss 2.8118 (2.6809)\tTop-5 Accuracy 67.296 (69.851)\t\n",
      "Epoch: [4][1100/5729]\tLoss 2.7716 (2.6824)\tTop-5 Accuracy 69.344 (69.828)\t\n",
      "Epoch: [4][1200/5729]\tLoss 2.6044 (2.6818)\tTop-5 Accuracy 71.074 (69.838)\t\n",
      "Epoch: [4][1300/5729]\tLoss 2.5647 (2.6834)\tTop-5 Accuracy 71.386 (69.814)\t\n",
      "Epoch: [4][1400/5729]\tLoss 2.6624 (2.6844)\tTop-5 Accuracy 69.709 (69.809)\t\n",
      "Epoch: [4][1500/5729]\tLoss 2.9771 (2.6856)\tTop-5 Accuracy 66.532 (69.789)\t\n",
      "Epoch: [4][1600/5729]\tLoss 2.7321 (2.6857)\tTop-5 Accuracy 69.712 (69.779)\t\n",
      "Epoch: [4][1700/5729]\tLoss 2.7320 (2.6866)\tTop-5 Accuracy 68.456 (69.768)\t\n",
      "Epoch: [4][1800/5729]\tLoss 2.7362 (2.6871)\tTop-5 Accuracy 69.854 (69.769)\t\n",
      "Epoch: [4][1900/5729]\tLoss 2.7273 (2.6880)\tTop-5 Accuracy 70.050 (69.760)\t\n",
      "Epoch: [4][2000/5729]\tLoss 2.5589 (2.6892)\tTop-5 Accuracy 71.815 (69.743)\t\n",
      "Epoch: [4][2100/5729]\tLoss 2.7662 (2.6898)\tTop-5 Accuracy 69.045 (69.733)\t\n",
      "Epoch: [4][2200/5729]\tLoss 2.6470 (2.6902)\tTop-5 Accuracy 70.464 (69.730)\t\n",
      "Epoch: [4][2300/5729]\tLoss 2.9006 (2.6910)\tTop-5 Accuracy 67.230 (69.718)\t\n",
      "Epoch: [4][2400/5729]\tLoss 2.6868 (2.6914)\tTop-5 Accuracy 68.706 (69.703)\t\n",
      "Epoch: [4][2500/5729]\tLoss 2.8459 (2.6917)\tTop-5 Accuracy 66.132 (69.703)\t\n",
      "Epoch: [4][2600/5729]\tLoss 2.6959 (2.6922)\tTop-5 Accuracy 69.231 (69.695)\t\n",
      "Epoch: [4][2700/5729]\tLoss 2.7191 (2.6930)\tTop-5 Accuracy 69.352 (69.682)\t\n",
      "Epoch: [4][2800/5729]\tLoss 2.6331 (2.6939)\tTop-5 Accuracy 71.188 (69.667)\t\n",
      "Epoch: [4][2900/5729]\tLoss 2.5180 (2.6941)\tTop-5 Accuracy 74.621 (69.670)\t\n",
      "Epoch: [4][3000/5729]\tLoss 2.9155 (2.6950)\tTop-5 Accuracy 65.880 (69.656)\t\n",
      "Epoch: [4][3100/5729]\tLoss 2.6718 (2.6956)\tTop-5 Accuracy 69.877 (69.650)\t\n",
      "Epoch: [4][3200/5729]\tLoss 2.8259 (2.6955)\tTop-5 Accuracy 68.155 (69.658)\t\n",
      "Epoch: [4][3300/5729]\tLoss 2.7616 (2.6958)\tTop-5 Accuracy 70.734 (69.658)\t\n",
      "Epoch: [4][3400/5729]\tLoss 2.8735 (2.6958)\tTop-5 Accuracy 67.269 (69.660)\t\n",
      "Epoch: [4][3500/5729]\tLoss 2.7942 (2.6961)\tTop-5 Accuracy 66.947 (69.659)\t\n",
      "Epoch: [4][3600/5729]\tLoss 2.5826 (2.6965)\tTop-5 Accuracy 71.198 (69.656)\t\n",
      "Epoch: [4][3700/5729]\tLoss 2.6852 (2.6969)\tTop-5 Accuracy 70.427 (69.656)\t\n",
      "Epoch: [4][3800/5729]\tLoss 2.7404 (2.6976)\tTop-5 Accuracy 69.455 (69.649)\t\n",
      "Epoch: [4][3900/5729]\tLoss 2.8924 (2.6982)\tTop-5 Accuracy 67.493 (69.644)\t\n",
      "Epoch: [4][4000/5729]\tLoss 2.6682 (2.6984)\tTop-5 Accuracy 70.294 (69.648)\t\n",
      "Epoch: [4][4100/5729]\tLoss 2.6762 (2.6986)\tTop-5 Accuracy 69.798 (69.647)\t\n",
      "Epoch: [4][4200/5729]\tLoss 2.8633 (2.6992)\tTop-5 Accuracy 66.828 (69.641)\t\n",
      "Epoch: [4][4300/5729]\tLoss 2.8085 (2.6994)\tTop-5 Accuracy 68.867 (69.638)\t\n",
      "Epoch: [4][4400/5729]\tLoss 2.7927 (2.6996)\tTop-5 Accuracy 69.461 (69.635)\t\n",
      "Epoch: [4][4500/5729]\tLoss 2.8532 (2.6997)\tTop-5 Accuracy 67.950 (69.631)\t\n",
      "Epoch: [4][4600/5729]\tLoss 2.7298 (2.6997)\tTop-5 Accuracy 68.979 (69.634)\t\n",
      "Epoch: [4][4700/5729]\tLoss 2.8573 (2.7002)\tTop-5 Accuracy 66.734 (69.630)\t\n",
      "Epoch: [4][4800/5729]\tLoss 2.7230 (2.7004)\tTop-5 Accuracy 70.000 (69.627)\t\n",
      "Epoch: [4][4900/5729]\tLoss 2.6351 (2.7005)\tTop-5 Accuracy 71.797 (69.624)\t\n",
      "Epoch: [4][5000/5729]\tLoss 2.7370 (2.7008)\tTop-5 Accuracy 69.369 (69.620)\t\n",
      "Epoch: [4][5100/5729]\tLoss 2.6886 (2.7011)\tTop-5 Accuracy 69.308 (69.618)\t\n",
      "Epoch: [4][5200/5729]\tLoss 2.9256 (2.7014)\tTop-5 Accuracy 66.977 (69.616)\t\n",
      "Epoch: [4][5300/5729]\tLoss 2.6773 (2.7017)\tTop-5 Accuracy 70.329 (69.612)\t\n",
      "Epoch: [4][5400/5729]\tLoss 2.8276 (2.7016)\tTop-5 Accuracy 67.664 (69.612)\t\n",
      "Epoch: [4][5500/5729]\tLoss 2.6314 (2.7019)\tTop-5 Accuracy 71.127 (69.608)\t\n",
      "Epoch: [4][5600/5729]\tLoss 2.5998 (2.7018)\tTop-5 Accuracy 71.485 (69.608)\t\n",
      "Epoch: [4][5700/5729]\tLoss 2.3974 (2.7019)\tTop-5 Accuracy 73.737 (69.606)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [47:24<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.541900\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 311760, 'reflen': 313617, 'guess': [311760, 271256, 230752, 190248], 'correct': [203104, 97768, 42102, 17881]}\n",
      "ratio: 0.9940787648628711\n",
      "Bleu_1: 0.648\n",
      "Bleu_2: 0.482\n",
      "Bleu_3: 0.348\n",
      "Bleu_4: 0.250\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.900\n",
      "Epoch 4:\tCIDEr Score: 0.899638820494321\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Predict: \n",
      "sebuah sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n",
      "Epoch: [5][0/5729]\tLoss 2.5942 (2.5942)\tTop-5 Accuracy 72.915 (72.915)\t\n",
      "Epoch: [5][100/5729]\tLoss 2.7098 (2.6081)\tTop-5 Accuracy 70.060 (70.738)\t\n",
      "Epoch: [5][200/5729]\tLoss 2.4974 (2.6050)\tTop-5 Accuracy 71.238 (70.854)\t\n",
      "Epoch: [5][300/5729]\tLoss 2.5800 (2.6081)\tTop-5 Accuracy 71.812 (70.766)\t\n",
      "Epoch: [5][400/5729]\tLoss 2.7142 (2.6092)\tTop-5 Accuracy 69.342 (70.737)\t\n",
      "Epoch: [5][500/5729]\tLoss 2.6380 (2.6107)\tTop-5 Accuracy 70.246 (70.724)\t\n",
      "Epoch: [5][600/5729]\tLoss 2.5668 (2.6137)\tTop-5 Accuracy 71.591 (70.707)\t\n",
      "Epoch: [5][700/5729]\tLoss 2.5756 (2.6152)\tTop-5 Accuracy 70.404 (70.702)\t\n",
      "Epoch: [5][800/5729]\tLoss 2.6502 (2.6147)\tTop-5 Accuracy 70.708 (70.722)\t\n",
      "Epoch: [5][900/5729]\tLoss 2.6246 (2.6148)\tTop-5 Accuracy 71.429 (70.727)\t\n",
      "Epoch: [5][1000/5729]\tLoss 2.7346 (2.6149)\tTop-5 Accuracy 68.493 (70.734)\t\n",
      "Epoch: [5][1100/5729]\tLoss 2.5745 (2.6180)\tTop-5 Accuracy 72.278 (70.706)\t\n",
      "Epoch: [5][1200/5729]\tLoss 2.6467 (2.6179)\tTop-5 Accuracy 68.986 (70.695)\t\n",
      "Epoch: [5][1300/5729]\tLoss 2.8226 (2.6200)\tTop-5 Accuracy 67.889 (70.668)\t\n",
      "Epoch: [5][1400/5729]\tLoss 2.6584 (2.6211)\tTop-5 Accuracy 69.505 (70.653)\t\n",
      "Epoch: [5][1500/5729]\tLoss 2.5227 (2.6216)\tTop-5 Accuracy 72.363 (70.645)\t\n",
      "Epoch: [5][1600/5729]\tLoss 2.7699 (2.6223)\tTop-5 Accuracy 68.786 (70.642)\t\n",
      "Epoch: [5][1700/5729]\tLoss 2.6303 (2.6237)\tTop-5 Accuracy 70.645 (70.617)\t\n",
      "Epoch: [5][1800/5729]\tLoss 2.6220 (2.6242)\tTop-5 Accuracy 71.225 (70.616)\t\n",
      "Epoch: [5][1900/5729]\tLoss 2.6504 (2.6250)\tTop-5 Accuracy 70.472 (70.606)\t\n",
      "Epoch: [5][2000/5729]\tLoss 2.4708 (2.6256)\tTop-5 Accuracy 72.368 (70.606)\t\n",
      "Epoch: [5][2100/5729]\tLoss 2.6705 (2.6269)\tTop-5 Accuracy 69.048 (70.592)\t\n",
      "Epoch: [5][2200/5729]\tLoss 2.4529 (2.6276)\tTop-5 Accuracy 71.975 (70.583)\t\n",
      "Epoch: [5][2300/5729]\tLoss 2.5247 (2.6275)\tTop-5 Accuracy 73.273 (70.584)\t\n",
      "Epoch: [5][2400/5729]\tLoss 2.6226 (2.6284)\tTop-5 Accuracy 70.465 (70.577)\t\n",
      "Epoch: [5][2500/5729]\tLoss 2.8707 (2.6294)\tTop-5 Accuracy 68.131 (70.564)\t\n",
      "Epoch: [5][2600/5729]\tLoss 2.7289 (2.6305)\tTop-5 Accuracy 69.253 (70.546)\t\n",
      "Epoch: [5][2700/5729]\tLoss 2.6723 (2.6314)\tTop-5 Accuracy 70.385 (70.536)\t\n",
      "Epoch: [5][2800/5729]\tLoss 2.5941 (2.6321)\tTop-5 Accuracy 69.886 (70.526)\t\n",
      "Epoch: [5][2900/5729]\tLoss 2.5290 (2.6330)\tTop-5 Accuracy 70.154 (70.522)\t\n",
      "Epoch: [5][3000/5729]\tLoss 2.6775 (2.6334)\tTop-5 Accuracy 69.254 (70.518)\t\n",
      "Epoch: [5][3100/5729]\tLoss 2.6496 (2.6338)\tTop-5 Accuracy 71.443 (70.513)\t\n",
      "Epoch: [5][3200/5729]\tLoss 2.6795 (2.6348)\tTop-5 Accuracy 69.143 (70.503)\t\n",
      "Epoch: [5][3300/5729]\tLoss 2.5874 (2.6355)\tTop-5 Accuracy 71.053 (70.494)\t\n",
      "Epoch: [5][3400/5729]\tLoss 2.7536 (2.6361)\tTop-5 Accuracy 69.854 (70.487)\t\n",
      "Epoch: [5][3500/5729]\tLoss 2.5247 (2.6365)\tTop-5 Accuracy 74.682 (70.482)\t\n",
      "Epoch: [5][3600/5729]\tLoss 2.6176 (2.6374)\tTop-5 Accuracy 71.700 (70.469)\t\n",
      "Epoch: [5][3700/5729]\tLoss 2.5950 (2.6379)\tTop-5 Accuracy 73.203 (70.463)\t\n",
      "Epoch: [5][3800/5729]\tLoss 2.6162 (2.6383)\tTop-5 Accuracy 72.719 (70.458)\t\n",
      "Epoch: [5][3900/5729]\tLoss 2.6313 (2.6389)\tTop-5 Accuracy 71.984 (70.452)\t\n",
      "Epoch: [5][4000/5729]\tLoss 2.6650 (2.6392)\tTop-5 Accuracy 70.900 (70.446)\t\n",
      "Epoch: [5][4100/5729]\tLoss 2.7323 (2.6397)\tTop-5 Accuracy 67.692 (70.439)\t\n",
      "Epoch: [5][4200/5729]\tLoss 2.5874 (2.6399)\tTop-5 Accuracy 70.611 (70.434)\t\n",
      "Epoch: [5][4300/5729]\tLoss 2.6631 (2.6405)\tTop-5 Accuracy 69.106 (70.429)\t\n",
      "Epoch: [5][4400/5729]\tLoss 2.6658 (2.6408)\tTop-5 Accuracy 69.904 (70.427)\t\n",
      "Epoch: [5][4500/5729]\tLoss 2.6684 (2.6409)\tTop-5 Accuracy 70.693 (70.425)\t\n",
      "Epoch: [5][4600/5729]\tLoss 2.5387 (2.6411)\tTop-5 Accuracy 72.925 (70.423)\t\n",
      "Epoch: [5][4700/5729]\tLoss 2.5326 (2.6413)\tTop-5 Accuracy 72.167 (70.422)\t\n",
      "Epoch: [5][4800/5729]\tLoss 2.5386 (2.6417)\tTop-5 Accuracy 72.267 (70.419)\t\n",
      "Epoch: [5][4900/5729]\tLoss 2.6763 (2.6419)\tTop-5 Accuracy 70.744 (70.421)\t\n",
      "Epoch: [5][5000/5729]\tLoss 2.8041 (2.6423)\tTop-5 Accuracy 69.381 (70.416)\t\n",
      "Epoch: [5][5100/5729]\tLoss 2.5348 (2.6424)\tTop-5 Accuracy 71.707 (70.416)\t\n",
      "Epoch: [5][5200/5729]\tLoss 2.7526 (2.6428)\tTop-5 Accuracy 69.254 (70.412)\t\n",
      "Epoch: [5][5300/5729]\tLoss 2.4798 (2.6431)\tTop-5 Accuracy 73.069 (70.408)\t\n",
      "Epoch: [5][5400/5729]\tLoss 2.5141 (2.6436)\tTop-5 Accuracy 71.914 (70.402)\t\n",
      "Epoch: [5][5500/5729]\tLoss 2.5534 (2.6441)\tTop-5 Accuracy 72.205 (70.400)\t\n",
      "Epoch: [5][5600/5729]\tLoss 2.6006 (2.6445)\tTop-5 Accuracy 71.291 (70.397)\t\n",
      "Epoch: [5][5700/5729]\tLoss 2.5891 (2.6449)\tTop-5 Accuracy 70.223 (70.391)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [47:04<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.541987\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 314718, 'reflen': 314843, 'guess': [314718, 274214, 233710, 193206], 'correct': [203099, 96448, 40762, 17431]}\n",
      "ratio: 0.999602976721728\n",
      "Bleu_1: 0.645\n",
      "Bleu_2: 0.476\n",
      "Bleu_3: 0.341\n",
      "Bleu_4: 0.244\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.893\n",
      "Epoch 5:\tCIDEr Score: 0.8929402969977082\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Predict: \n",
      "sebuah sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n",
      "\n",
      "DECAYING learning rate.\n",
      "The new learning rate is 0.000320\n",
      "\n",
      "Epoch: [6][0/5729]\tLoss 2.5678 (2.5678)\tTop-5 Accuracy 70.965 (70.965)\t\n",
      "Epoch: [6][100/5729]\tLoss 2.6210 (2.5394)\tTop-5 Accuracy 71.279 (71.869)\t\n",
      "Epoch: [6][200/5729]\tLoss 2.5257 (2.5310)\tTop-5 Accuracy 73.154 (71.904)\t\n",
      "Epoch: [6][300/5729]\tLoss 2.5936 (2.5354)\tTop-5 Accuracy 71.484 (71.889)\t\n",
      "Epoch: [6][400/5729]\tLoss 2.5773 (2.5341)\tTop-5 Accuracy 72.065 (71.941)\t\n",
      "Epoch: [6][500/5729]\tLoss 2.5429 (2.5341)\tTop-5 Accuracy 70.588 (71.935)\t\n",
      "Epoch: [6][600/5729]\tLoss 2.6136 (2.5373)\tTop-5 Accuracy 69.547 (71.878)\t\n",
      "Epoch: [6][700/5729]\tLoss 2.6617 (2.5393)\tTop-5 Accuracy 70.631 (71.858)\t\n",
      "Epoch: [6][800/5729]\tLoss 2.5241 (2.5404)\tTop-5 Accuracy 71.702 (71.845)\t\n",
      "Epoch: [6][900/5729]\tLoss 2.5272 (2.5405)\tTop-5 Accuracy 73.353 (71.845)\t\n",
      "Epoch: [6][1000/5729]\tLoss 2.4964 (2.5422)\tTop-5 Accuracy 72.995 (71.812)\t\n",
      "Epoch: [6][1100/5729]\tLoss 2.5951 (2.5432)\tTop-5 Accuracy 72.095 (71.806)\t\n",
      "Epoch: [6][1200/5729]\tLoss 2.5352 (2.5428)\tTop-5 Accuracy 72.155 (71.813)\t\n",
      "Epoch: [6][1300/5729]\tLoss 2.5578 (2.5439)\tTop-5 Accuracy 70.479 (71.792)\t\n",
      "Epoch: [6][1400/5729]\tLoss 2.5224 (2.5434)\tTop-5 Accuracy 73.477 (71.795)\t\n",
      "Epoch: [6][1500/5729]\tLoss 2.5278 (2.5441)\tTop-5 Accuracy 71.314 (71.786)\t\n",
      "Epoch: [6][1600/5729]\tLoss 2.6471 (2.5447)\tTop-5 Accuracy 68.969 (71.773)\t\n",
      "Epoch: [6][1700/5729]\tLoss 2.4964 (2.5458)\tTop-5 Accuracy 73.473 (71.758)\t\n",
      "Epoch: [6][1800/5729]\tLoss 2.3934 (2.5478)\tTop-5 Accuracy 73.992 (71.729)\t\n",
      "Epoch: [6][1900/5729]\tLoss 2.5483 (2.5488)\tTop-5 Accuracy 71.318 (71.716)\t\n",
      "Epoch: [6][2000/5729]\tLoss 2.6738 (2.5499)\tTop-5 Accuracy 70.268 (71.693)\t\n",
      "Epoch: [6][2100/5729]\tLoss 2.5573 (2.5503)\tTop-5 Accuracy 72.287 (71.695)\t\n",
      "Epoch: [6][2200/5729]\tLoss 2.5815 (2.5501)\tTop-5 Accuracy 72.050 (71.694)\t\n",
      "Epoch: [6][2300/5729]\tLoss 2.4345 (2.5516)\tTop-5 Accuracy 73.430 (71.670)\t\n",
      "Epoch: [6][2400/5729]\tLoss 2.4841 (2.5524)\tTop-5 Accuracy 73.513 (71.661)\t\n",
      "Epoch: [6][2500/5729]\tLoss 2.5633 (2.5532)\tTop-5 Accuracy 70.934 (71.650)\t\n",
      "Epoch: [6][2600/5729]\tLoss 2.5499 (2.5537)\tTop-5 Accuracy 72.515 (71.646)\t\n",
      "Epoch: [6][2700/5729]\tLoss 2.5815 (2.5544)\tTop-5 Accuracy 71.727 (71.639)\t\n",
      "Epoch: [6][2800/5729]\tLoss 2.6792 (2.5546)\tTop-5 Accuracy 69.752 (71.636)\t\n",
      "Epoch: [6][2900/5729]\tLoss 2.5404 (2.5555)\tTop-5 Accuracy 72.854 (71.626)\t\n",
      "Epoch: [6][3000/5729]\tLoss 2.4739 (2.5561)\tTop-5 Accuracy 73.396 (71.619)\t\n",
      "Epoch: [6][3100/5729]\tLoss 2.5062 (2.5565)\tTop-5 Accuracy 71.957 (71.617)\t\n",
      "Epoch: [6][3200/5729]\tLoss 2.5632 (2.5568)\tTop-5 Accuracy 72.551 (71.615)\t\n",
      "Epoch: [6][3300/5729]\tLoss 2.5290 (2.5576)\tTop-5 Accuracy 71.831 (71.602)\t\n",
      "Epoch: [6][3400/5729]\tLoss 2.4990 (2.5582)\tTop-5 Accuracy 71.810 (71.596)\t\n",
      "Epoch: [6][3500/5729]\tLoss 2.5988 (2.5590)\tTop-5 Accuracy 71.324 (71.586)\t\n",
      "Epoch: [6][3600/5729]\tLoss 2.5172 (2.5592)\tTop-5 Accuracy 72.467 (71.585)\t\n",
      "Epoch: [6][3700/5729]\tLoss 2.4495 (2.5602)\tTop-5 Accuracy 71.827 (71.569)\t\n",
      "Epoch: [6][3800/5729]\tLoss 2.5550 (2.5610)\tTop-5 Accuracy 72.244 (71.558)\t\n",
      "Epoch: [6][3900/5729]\tLoss 2.5284 (2.5614)\tTop-5 Accuracy 72.531 (71.553)\t\n",
      "Epoch: [6][4000/5729]\tLoss 2.5397 (2.5615)\tTop-5 Accuracy 72.141 (71.556)\t\n",
      "Epoch: [6][4100/5729]\tLoss 2.5760 (2.5617)\tTop-5 Accuracy 72.216 (71.555)\t\n",
      "Epoch: [6][4200/5729]\tLoss 2.7389 (2.5625)\tTop-5 Accuracy 70.107 (71.546)\t\n",
      "Epoch: [6][4300/5729]\tLoss 2.7230 (2.5627)\tTop-5 Accuracy 69.332 (71.543)\t\n",
      "Epoch: [6][4400/5729]\tLoss 2.6846 (2.5635)\tTop-5 Accuracy 69.298 (71.536)\t\n",
      "Epoch: [6][4500/5729]\tLoss 2.6608 (2.5638)\tTop-5 Accuracy 70.846 (71.535)\t\n",
      "Epoch: [6][4600/5729]\tLoss 2.5682 (2.5642)\tTop-5 Accuracy 72.052 (71.527)\t\n",
      "Epoch: [6][4700/5729]\tLoss 2.6219 (2.5647)\tTop-5 Accuracy 70.367 (71.520)\t\n",
      "Epoch: [6][4800/5729]\tLoss 2.5139 (2.5649)\tTop-5 Accuracy 71.545 (71.518)\t\n",
      "Epoch: [6][4900/5729]\tLoss 2.3195 (2.5650)\tTop-5 Accuracy 75.398 (71.515)\t\n",
      "Epoch: [6][5000/5729]\tLoss 2.5901 (2.5652)\tTop-5 Accuracy 70.676 (71.514)\t\n",
      "Epoch: [6][5100/5729]\tLoss 2.6432 (2.5655)\tTop-5 Accuracy 70.804 (71.512)\t\n",
      "Epoch: [6][5200/5729]\tLoss 2.6379 (2.5660)\tTop-5 Accuracy 69.483 (71.506)\t\n",
      "Epoch: [6][5300/5729]\tLoss 2.4040 (2.5667)\tTop-5 Accuracy 73.394 (71.497)\t\n",
      "Epoch: [6][5400/5729]\tLoss 2.5186 (2.5670)\tTop-5 Accuracy 73.985 (71.496)\t\n",
      "Epoch: [6][5500/5729]\tLoss 2.4363 (2.5672)\tTop-5 Accuracy 72.575 (71.496)\t\n",
      "Epoch: [6][5600/5729]\tLoss 2.6186 (2.5674)\tTop-5 Accuracy 70.582 (71.493)\t\n",
      "Epoch: [6][5700/5729]\tLoss 2.4760 (2.5681)\tTop-5 Accuracy 74.050 (71.480)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [46:33<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.561079\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 309788, 'reflen': 312365, 'guess': [309788, 269284, 228780, 188276], 'correct': [203576, 97291, 41532, 17457]}\n",
      "ratio: 0.9917500360155556\n",
      "Bleu_1: 0.652\n",
      "Bleu_2: 0.483\n",
      "Bleu_3: 0.348\n",
      "Bleu_4: 0.249\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.903\n",
      "Epoch 6:\tCIDEr Score: 0.9033666380065309\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Predict: \n",
      "sebuah sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n",
      "Epoch: [7][0/5729]\tLoss 2.4159 (2.4159)\tTop-5 Accuracy 74.713 (74.713)\t\n",
      "Epoch: [7][100/5729]\tLoss 2.3843 (2.4603)\tTop-5 Accuracy 72.771 (72.897)\t\n",
      "Epoch: [7][200/5729]\tLoss 2.4300 (2.4648)\tTop-5 Accuracy 73.456 (72.816)\t\n",
      "Epoch: [7][300/5729]\tLoss 2.4127 (2.4665)\tTop-5 Accuracy 72.029 (72.798)\t\n",
      "Epoch: [7][400/5729]\tLoss 2.5952 (2.4766)\tTop-5 Accuracy 71.633 (72.619)\t\n",
      "Epoch: [7][500/5729]\tLoss 2.4923 (2.4778)\tTop-5 Accuracy 71.270 (72.612)\t\n",
      "Epoch: [7][600/5729]\tLoss 2.3574 (2.4799)\tTop-5 Accuracy 74.427 (72.570)\t\n",
      "Epoch: [7][700/5729]\tLoss 2.3022 (2.4795)\tTop-5 Accuracy 76.477 (72.574)\t\n",
      "Epoch: [7][800/5729]\tLoss 2.5951 (2.4835)\tTop-5 Accuracy 72.211 (72.528)\t\n",
      "Epoch: [7][900/5729]\tLoss 2.3472 (2.4854)\tTop-5 Accuracy 76.290 (72.530)\t\n",
      "Epoch: [7][1000/5729]\tLoss 2.6590 (2.4884)\tTop-5 Accuracy 69.971 (72.493)\t\n",
      "Epoch: [7][1100/5729]\tLoss 2.5970 (2.4911)\tTop-5 Accuracy 71.954 (72.453)\t\n",
      "Epoch: [7][1200/5729]\tLoss 2.3643 (2.4932)\tTop-5 Accuracy 74.948 (72.411)\t\n",
      "Epoch: [7][1300/5729]\tLoss 2.5550 (2.4943)\tTop-5 Accuracy 72.571 (72.392)\t\n",
      "Epoch: [7][1400/5729]\tLoss 2.4487 (2.4952)\tTop-5 Accuracy 72.682 (72.384)\t\n",
      "Epoch: [7][1500/5729]\tLoss 2.5278 (2.4963)\tTop-5 Accuracy 71.668 (72.374)\t\n",
      "Epoch: [7][1600/5729]\tLoss 2.4651 (2.4973)\tTop-5 Accuracy 74.423 (72.364)\t\n",
      "Epoch: [7][1700/5729]\tLoss 2.4305 (2.4975)\tTop-5 Accuracy 73.843 (72.375)\t\n",
      "Epoch: [7][1800/5729]\tLoss 2.5945 (2.4976)\tTop-5 Accuracy 72.185 (72.388)\t\n",
      "Epoch: [7][1900/5729]\tLoss 2.7317 (2.4983)\tTop-5 Accuracy 68.133 (72.380)\t\n",
      "Epoch: [7][2000/5729]\tLoss 2.6096 (2.4993)\tTop-5 Accuracy 71.374 (72.364)\t\n",
      "Epoch: [7][2100/5729]\tLoss 2.3999 (2.5003)\tTop-5 Accuracy 73.977 (72.352)\t\n",
      "Epoch: [7][2200/5729]\tLoss 2.4676 (2.5005)\tTop-5 Accuracy 71.914 (72.353)\t\n",
      "Epoch: [7][2300/5729]\tLoss 2.5408 (2.5017)\tTop-5 Accuracy 70.523 (72.335)\t\n",
      "Epoch: [7][2400/5729]\tLoss 2.5483 (2.5018)\tTop-5 Accuracy 70.964 (72.341)\t\n",
      "Epoch: [7][2500/5729]\tLoss 2.5278 (2.5028)\tTop-5 Accuracy 70.929 (72.331)\t\n",
      "Epoch: [7][2600/5729]\tLoss 2.5189 (2.5030)\tTop-5 Accuracy 72.719 (72.331)\t\n",
      "Epoch: [7][2700/5729]\tLoss 2.4274 (2.5043)\tTop-5 Accuracy 73.121 (72.305)\t\n",
      "Epoch: [7][2800/5729]\tLoss 2.5050 (2.5051)\tTop-5 Accuracy 72.638 (72.293)\t\n",
      "Epoch: [7][2900/5729]\tLoss 2.5125 (2.5054)\tTop-5 Accuracy 72.417 (72.292)\t\n",
      "Epoch: [7][3000/5729]\tLoss 2.5008 (2.5066)\tTop-5 Accuracy 72.390 (72.279)\t\n",
      "Epoch: [7][3100/5729]\tLoss 2.4871 (2.5075)\tTop-5 Accuracy 72.620 (72.267)\t\n",
      "Epoch: [7][3200/5729]\tLoss 2.5306 (2.5083)\tTop-5 Accuracy 72.378 (72.257)\t\n",
      "Epoch: [7][3300/5729]\tLoss 2.4645 (2.5091)\tTop-5 Accuracy 71.813 (72.241)\t\n",
      "Epoch: [7][3400/5729]\tLoss 2.4117 (2.5097)\tTop-5 Accuracy 73.843 (72.236)\t\n",
      "Epoch: [7][3500/5729]\tLoss 2.5919 (2.5102)\tTop-5 Accuracy 69.856 (72.235)\t\n",
      "Epoch: [7][3600/5729]\tLoss 2.3605 (2.5108)\tTop-5 Accuracy 74.256 (72.226)\t\n",
      "Epoch: [7][3700/5729]\tLoss 2.6516 (2.5115)\tTop-5 Accuracy 70.636 (72.219)\t\n",
      "Epoch: [7][3800/5729]\tLoss 2.5757 (2.5120)\tTop-5 Accuracy 71.154 (72.215)\t\n",
      "Epoch: [7][3900/5729]\tLoss 2.5098 (2.5127)\tTop-5 Accuracy 72.495 (72.207)\t\n",
      "Epoch: [7][4000/5729]\tLoss 2.4641 (2.5133)\tTop-5 Accuracy 72.378 (72.202)\t\n",
      "Epoch: [7][4100/5729]\tLoss 2.4927 (2.5138)\tTop-5 Accuracy 72.530 (72.193)\t\n",
      "Epoch: [7][4200/5729]\tLoss 2.3444 (2.5147)\tTop-5 Accuracy 74.877 (72.181)\t\n",
      "Epoch: [7][4300/5729]\tLoss 2.3239 (2.5148)\tTop-5 Accuracy 74.831 (72.182)\t\n",
      "Epoch: [7][4400/5729]\tLoss 2.6154 (2.5156)\tTop-5 Accuracy 70.334 (72.174)\t\n",
      "Epoch: [7][4500/5729]\tLoss 2.5251 (2.5161)\tTop-5 Accuracy 72.134 (72.168)\t\n",
      "Epoch: [7][4600/5729]\tLoss 2.4123 (2.5164)\tTop-5 Accuracy 74.776 (72.166)\t\n",
      "Epoch: [7][4700/5729]\tLoss 2.4753 (2.5168)\tTop-5 Accuracy 73.555 (72.163)\t\n",
      "Epoch: [7][4800/5729]\tLoss 2.7341 (2.5174)\tTop-5 Accuracy 69.590 (72.156)\t\n",
      "Epoch: [7][4900/5729]\tLoss 2.5188 (2.5178)\tTop-5 Accuracy 72.465 (72.154)\t\n",
      "Epoch: [7][5000/5729]\tLoss 2.6111 (2.5181)\tTop-5 Accuracy 70.633 (72.150)\t\n",
      "Epoch: [7][5100/5729]\tLoss 2.4777 (2.5187)\tTop-5 Accuracy 72.665 (72.145)\t\n",
      "Epoch: [7][5200/5729]\tLoss 2.6710 (2.5193)\tTop-5 Accuracy 70.194 (72.137)\t\n",
      "Epoch: [7][5300/5729]\tLoss 2.5138 (2.5198)\tTop-5 Accuracy 72.906 (72.131)\t\n",
      "Epoch: [7][5400/5729]\tLoss 2.4690 (2.5202)\tTop-5 Accuracy 71.582 (72.126)\t\n",
      "Epoch: [7][5500/5729]\tLoss 2.5162 (2.5206)\tTop-5 Accuracy 72.754 (72.122)\t\n",
      "Epoch: [7][5600/5729]\tLoss 2.4409 (2.5211)\tTop-5 Accuracy 75.152 (72.115)\t\n",
      "Epoch: [7][5700/5729]\tLoss 2.6843 (2.5216)\tTop-5 Accuracy 69.021 (72.109)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [47:02<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.570997\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 313717, 'reflen': 314815, 'guess': [313717, 273213, 232709, 192206], 'correct': [202589, 95093, 40409, 17032]}\n",
      "ratio: 0.9965122373457396\n",
      "Bleu_1: 0.644\n",
      "Bleu_2: 0.472\n",
      "Bleu_3: 0.338\n",
      "Bleu_4: 0.242\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.885\n",
      "Epoch 7:\tCIDEr Score: 0.8850022203548268\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Predict: \n",
      "sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n",
      "Epoch: [8][0/5729]\tLoss 2.5875 (2.5875)\tTop-5 Accuracy 71.001 (71.001)\t\n",
      "Epoch: [8][100/5729]\tLoss 2.6307 (2.4320)\tTop-5 Accuracy 70.549 (73.237)\t\n",
      "Epoch: [8][200/5729]\tLoss 2.3776 (2.4257)\tTop-5 Accuracy 74.472 (73.361)\t\n",
      "Epoch: [8][300/5729]\tLoss 2.4352 (2.4282)\tTop-5 Accuracy 74.032 (73.359)\t\n",
      "Epoch: [8][400/5729]\tLoss 2.4405 (2.4326)\tTop-5 Accuracy 73.367 (73.294)\t\n",
      "Epoch: [8][500/5729]\tLoss 2.3236 (2.4328)\tTop-5 Accuracy 74.487 (73.280)\t\n",
      "Epoch: [8][600/5729]\tLoss 2.3363 (2.4361)\tTop-5 Accuracy 73.971 (73.240)\t\n",
      "Epoch: [8][700/5729]\tLoss 2.4350 (2.4376)\tTop-5 Accuracy 72.598 (73.221)\t\n",
      "Epoch: [8][800/5729]\tLoss 2.4172 (2.4387)\tTop-5 Accuracy 72.887 (73.214)\t\n",
      "Epoch: [8][900/5729]\tLoss 2.4591 (2.4399)\tTop-5 Accuracy 72.068 (73.196)\t\n",
      "Epoch: [8][1000/5729]\tLoss 2.3943 (2.4428)\tTop-5 Accuracy 74.364 (73.151)\t\n",
      "Epoch: [8][1100/5729]\tLoss 2.3810 (2.4437)\tTop-5 Accuracy 74.749 (73.142)\t\n",
      "Epoch: [8][1200/5729]\tLoss 2.3991 (2.4456)\tTop-5 Accuracy 74.400 (73.115)\t\n",
      "Epoch: [8][1300/5729]\tLoss 2.5039 (2.4469)\tTop-5 Accuracy 72.944 (73.103)\t\n",
      "Epoch: [8][1400/5729]\tLoss 2.4113 (2.4470)\tTop-5 Accuracy 74.391 (73.101)\t\n",
      "Epoch: [8][1500/5729]\tLoss 2.4180 (2.4492)\tTop-5 Accuracy 73.220 (73.074)\t\n",
      "Epoch: [8][1600/5729]\tLoss 2.4774 (2.4497)\tTop-5 Accuracy 72.745 (73.075)\t\n",
      "Epoch: [8][1700/5729]\tLoss 2.6342 (2.4509)\tTop-5 Accuracy 70.771 (73.063)\t\n",
      "Epoch: [8][1800/5729]\tLoss 2.3699 (2.4526)\tTop-5 Accuracy 74.480 (73.037)\t\n",
      "Epoch: [8][1900/5729]\tLoss 2.3891 (2.4543)\tTop-5 Accuracy 74.431 (73.014)\t\n",
      "Epoch: [8][2000/5729]\tLoss 2.2948 (2.4554)\tTop-5 Accuracy 75.837 (72.995)\t\n",
      "Epoch: [8][2100/5729]\tLoss 2.4989 (2.4562)\tTop-5 Accuracy 73.425 (72.987)\t\n",
      "Epoch: [8][2200/5729]\tLoss 2.6573 (2.4570)\tTop-5 Accuracy 69.626 (72.972)\t\n",
      "Epoch: [8][2300/5729]\tLoss 2.3433 (2.4582)\tTop-5 Accuracy 74.531 (72.956)\t\n",
      "Epoch: [8][2400/5729]\tLoss 2.4879 (2.4598)\tTop-5 Accuracy 72.828 (72.938)\t\n",
      "Epoch: [8][2500/5729]\tLoss 2.3093 (2.4606)\tTop-5 Accuracy 75.205 (72.933)\t\n",
      "Epoch: [8][2600/5729]\tLoss 2.5596 (2.4616)\tTop-5 Accuracy 72.080 (72.918)\t\n",
      "Epoch: [8][2700/5729]\tLoss 2.4055 (2.4620)\tTop-5 Accuracy 74.704 (72.913)\t\n",
      "Epoch: [8][2800/5729]\tLoss 2.5654 (2.4624)\tTop-5 Accuracy 70.525 (72.908)\t\n",
      "Epoch: [8][2900/5729]\tLoss 2.4939 (2.4632)\tTop-5 Accuracy 71.590 (72.899)\t\n",
      "Epoch: [8][3000/5729]\tLoss 2.4386 (2.4639)\tTop-5 Accuracy 71.832 (72.891)\t\n",
      "Epoch: [8][3100/5729]\tLoss 2.4107 (2.4645)\tTop-5 Accuracy 74.487 (72.888)\t\n",
      "Epoch: [8][3200/5729]\tLoss 2.7607 (2.4657)\tTop-5 Accuracy 69.962 (72.875)\t\n",
      "Epoch: [8][3300/5729]\tLoss 2.3015 (2.4665)\tTop-5 Accuracy 75.439 (72.863)\t\n",
      "Epoch: [8][3400/5729]\tLoss 2.4780 (2.4672)\tTop-5 Accuracy 71.688 (72.854)\t\n",
      "Epoch: [8][3500/5729]\tLoss 2.3952 (2.4683)\tTop-5 Accuracy 73.872 (72.839)\t\n",
      "Epoch: [8][3600/5729]\tLoss 2.5914 (2.4688)\tTop-5 Accuracy 69.925 (72.831)\t\n",
      "Epoch: [8][3700/5729]\tLoss 2.4927 (2.4695)\tTop-5 Accuracy 72.340 (72.824)\t\n",
      "Epoch: [8][3800/5729]\tLoss 2.5424 (2.4699)\tTop-5 Accuracy 73.327 (72.827)\t\n",
      "Epoch: [8][3900/5729]\tLoss 2.4876 (2.4705)\tTop-5 Accuracy 72.040 (72.818)\t\n",
      "Epoch: [8][4000/5729]\tLoss 2.4488 (2.4712)\tTop-5 Accuracy 71.938 (72.808)\t\n",
      "Epoch: [8][4100/5729]\tLoss 2.3815 (2.4717)\tTop-5 Accuracy 74.266 (72.801)\t\n",
      "Epoch: [8][4200/5729]\tLoss 2.4769 (2.4725)\tTop-5 Accuracy 72.591 (72.790)\t\n",
      "Epoch: [8][4300/5729]\tLoss 2.5930 (2.4731)\tTop-5 Accuracy 73.136 (72.780)\t\n",
      "Epoch: [8][4400/5729]\tLoss 2.2980 (2.4740)\tTop-5 Accuracy 75.202 (72.772)\t\n",
      "Epoch: [8][4500/5729]\tLoss 2.2964 (2.4747)\tTop-5 Accuracy 75.654 (72.763)\t\n",
      "Epoch: [8][4600/5729]\tLoss 2.3835 (2.4755)\tTop-5 Accuracy 74.879 (72.752)\t\n",
      "Epoch: [8][4700/5729]\tLoss 2.5384 (2.4757)\tTop-5 Accuracy 71.105 (72.747)\t\n",
      "Epoch: [8][4800/5729]\tLoss 2.6904 (2.4763)\tTop-5 Accuracy 70.197 (72.737)\t\n",
      "Epoch: [8][4900/5729]\tLoss 2.5414 (2.4770)\tTop-5 Accuracy 73.137 (72.730)\t\n",
      "Epoch: [8][5000/5729]\tLoss 2.5906 (2.4777)\tTop-5 Accuracy 72.380 (72.723)\t\n",
      "Epoch: [8][5100/5729]\tLoss 2.5618 (2.4783)\tTop-5 Accuracy 71.482 (72.716)\t\n",
      "Epoch: [8][5200/5729]\tLoss 2.5726 (2.4792)\tTop-5 Accuracy 70.096 (72.701)\t\n",
      "Epoch: [8][5300/5729]\tLoss 2.7354 (2.4797)\tTop-5 Accuracy 69.252 (72.696)\t\n",
      "Epoch: [8][5400/5729]\tLoss 2.4565 (2.4803)\tTop-5 Accuracy 73.327 (72.688)\t\n",
      "Epoch: [8][5500/5729]\tLoss 2.6357 (2.4810)\tTop-5 Accuracy 70.082 (72.678)\t\n",
      "Epoch: [8][5600/5729]\tLoss 2.4795 (2.4814)\tTop-5 Accuracy 73.265 (72.672)\t\n",
      "Epoch: [8][5700/5729]\tLoss 2.5100 (2.4818)\tTop-5 Accuracy 74.093 (72.668)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [46:39<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.526997\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 311526, 'reflen': 313734, 'guess': [311526, 271022, 230518, 190014], 'correct': [202938, 96516, 41180, 17596]}\n",
      "ratio: 0.9929621909005687\n",
      "Bleu_1: 0.647\n",
      "Bleu_2: 0.478\n",
      "Bleu_3: 0.344\n",
      "Bleu_4: 0.247\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.892\n",
      "Epoch 8:\tCIDEr Score: 0.8920561695740918\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Predict: \n",
      "sebuah sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n",
      "\n",
      "DECAYING learning rate.\n",
      "The new learning rate is 0.000256\n",
      "\n",
      "Epoch: [9][0/5729]\tLoss 2.2291 (2.2291)\tTop-5 Accuracy 77.196 (77.196)\t\n",
      "Epoch: [9][100/5729]\tLoss 2.2580 (2.3798)\tTop-5 Accuracy 76.572 (74.083)\t\n",
      "Epoch: [9][200/5729]\tLoss 2.5705 (2.3818)\tTop-5 Accuracy 71.570 (74.039)\t\n",
      "Epoch: [9][300/5729]\tLoss 2.2621 (2.3818)\tTop-5 Accuracy 74.070 (74.021)\t\n",
      "Epoch: [9][400/5729]\tLoss 2.3359 (2.3846)\tTop-5 Accuracy 74.316 (73.964)\t\n",
      "Epoch: [9][500/5729]\tLoss 2.3475 (2.3818)\tTop-5 Accuracy 74.641 (74.010)\t\n",
      "Epoch: [9][600/5729]\tLoss 2.3871 (2.3818)\tTop-5 Accuracy 74.274 (74.022)\t\n",
      "Epoch: [9][700/5729]\tLoss 2.3110 (2.3839)\tTop-5 Accuracy 75.394 (73.992)\t\n",
      "Epoch: [9][800/5729]\tLoss 2.2966 (2.3844)\tTop-5 Accuracy 75.814 (73.978)\t\n",
      "Epoch: [9][900/5729]\tLoss 2.3004 (2.3854)\tTop-5 Accuracy 75.251 (73.964)\t\n",
      "Epoch: [9][1000/5729]\tLoss 2.4467 (2.3866)\tTop-5 Accuracy 72.173 (73.949)\t\n",
      "Epoch: [9][1100/5729]\tLoss 2.4537 (2.3868)\tTop-5 Accuracy 72.991 (73.945)\t\n",
      "Epoch: [9][1200/5729]\tLoss 2.2748 (2.3868)\tTop-5 Accuracy 74.790 (73.941)\t\n",
      "Epoch: [9][1300/5729]\tLoss 2.2253 (2.3875)\tTop-5 Accuracy 77.022 (73.928)\t\n",
      "Epoch: [9][1400/5729]\tLoss 2.2270 (2.3878)\tTop-5 Accuracy 75.124 (73.917)\t\n",
      "Epoch: [9][1500/5729]\tLoss 2.5370 (2.3890)\tTop-5 Accuracy 70.650 (73.905)\t\n",
      "Epoch: [9][1600/5729]\tLoss 2.4854 (2.3899)\tTop-5 Accuracy 71.905 (73.905)\t\n",
      "Epoch: [9][1700/5729]\tLoss 2.3243 (2.3911)\tTop-5 Accuracy 74.776 (73.886)\t\n",
      "Epoch: [9][1800/5729]\tLoss 2.5366 (2.3924)\tTop-5 Accuracy 73.069 (73.869)\t\n",
      "Epoch: [9][1900/5729]\tLoss 2.4885 (2.3932)\tTop-5 Accuracy 72.826 (73.863)\t\n",
      "Epoch: [9][2000/5729]\tLoss 2.5689 (2.3938)\tTop-5 Accuracy 70.797 (73.854)\t\n",
      "Epoch: [9][2100/5729]\tLoss 2.3465 (2.3948)\tTop-5 Accuracy 74.676 (73.837)\t\n",
      "Epoch: [9][2200/5729]\tLoss 2.2629 (2.3960)\tTop-5 Accuracy 74.203 (73.824)\t\n",
      "Epoch: [9][2300/5729]\tLoss 2.4273 (2.3962)\tTop-5 Accuracy 74.025 (73.825)\t\n",
      "Epoch: [9][2400/5729]\tLoss 2.4151 (2.3968)\tTop-5 Accuracy 74.161 (73.819)\t\n",
      "Epoch: [9][2500/5729]\tLoss 2.4759 (2.3967)\tTop-5 Accuracy 72.983 (73.824)\t\n",
      "Epoch: [9][2600/5729]\tLoss 2.4069 (2.3979)\tTop-5 Accuracy 74.257 (73.808)\t\n",
      "Epoch: [9][2700/5729]\tLoss 2.3691 (2.3981)\tTop-5 Accuracy 73.478 (73.805)\t\n",
      "Epoch: [9][2800/5729]\tLoss 2.3219 (2.3993)\tTop-5 Accuracy 75.937 (73.791)\t\n",
      "Epoch: [9][2900/5729]\tLoss 2.3570 (2.3997)\tTop-5 Accuracy 75.761 (73.790)\t\n",
      "Epoch: [9][3000/5729]\tLoss 2.4963 (2.4009)\tTop-5 Accuracy 73.092 (73.774)\t\n",
      "Epoch: [9][3100/5729]\tLoss 2.4328 (2.4019)\tTop-5 Accuracy 73.510 (73.764)\t\n",
      "Epoch: [9][3200/5729]\tLoss 2.4635 (2.4031)\tTop-5 Accuracy 72.835 (73.744)\t\n",
      "Epoch: [9][3300/5729]\tLoss 2.4538 (2.4041)\tTop-5 Accuracy 74.848 (73.727)\t\n",
      "Epoch: [9][3400/5729]\tLoss 2.3489 (2.4049)\tTop-5 Accuracy 74.519 (73.717)\t\n",
      "Epoch: [9][3500/5729]\tLoss 2.4473 (2.4055)\tTop-5 Accuracy 73.577 (73.706)\t\n",
      "Epoch: [9][3600/5729]\tLoss 2.4719 (2.4062)\tTop-5 Accuracy 73.286 (73.699)\t\n",
      "Epoch: [9][3700/5729]\tLoss 2.5012 (2.4071)\tTop-5 Accuracy 72.604 (73.686)\t\n",
      "Epoch: [9][3800/5729]\tLoss 2.3918 (2.4078)\tTop-5 Accuracy 74.652 (73.673)\t\n",
      "Epoch: [9][3900/5729]\tLoss 2.2630 (2.4084)\tTop-5 Accuracy 75.307 (73.666)\t\n",
      "Epoch: [9][4000/5729]\tLoss 2.3823 (2.4091)\tTop-5 Accuracy 74.430 (73.658)\t\n",
      "Epoch: [9][4100/5729]\tLoss 2.4360 (2.4095)\tTop-5 Accuracy 74.382 (73.653)\t\n",
      "Epoch: [9][4200/5729]\tLoss 2.3468 (2.4097)\tTop-5 Accuracy 73.178 (73.652)\t\n",
      "Epoch: [9][4300/5729]\tLoss 2.4308 (2.4105)\tTop-5 Accuracy 72.233 (73.639)\t\n",
      "Epoch: [9][4400/5729]\tLoss 2.4448 (2.4117)\tTop-5 Accuracy 72.847 (73.625)\t\n",
      "Epoch: [9][4500/5729]\tLoss 2.5171 (2.4120)\tTop-5 Accuracy 72.983 (73.621)\t\n",
      "Epoch: [9][4600/5729]\tLoss 2.3310 (2.4125)\tTop-5 Accuracy 74.303 (73.616)\t\n",
      "Epoch: [9][4700/5729]\tLoss 2.3874 (2.4128)\tTop-5 Accuracy 74.087 (73.610)\t\n",
      "Epoch: [9][4800/5729]\tLoss 2.4816 (2.4135)\tTop-5 Accuracy 72.844 (73.602)\t\n",
      "Epoch: [9][4900/5729]\tLoss 2.5170 (2.4142)\tTop-5 Accuracy 71.358 (73.593)\t\n",
      "Epoch: [9][5000/5729]\tLoss 2.2582 (2.4146)\tTop-5 Accuracy 75.752 (73.587)\t\n",
      "Epoch: [9][5100/5729]\tLoss 2.2718 (2.4152)\tTop-5 Accuracy 75.695 (73.581)\t\n",
      "Epoch: [9][5200/5729]\tLoss 2.4643 (2.4159)\tTop-5 Accuracy 72.622 (73.569)\t\n",
      "Epoch: [9][5300/5729]\tLoss 2.4412 (2.4167)\tTop-5 Accuracy 73.719 (73.562)\t\n",
      "Epoch: [9][5400/5729]\tLoss 2.4913 (2.4173)\tTop-5 Accuracy 71.982 (73.551)\t\n",
      "Epoch: [9][5500/5729]\tLoss 2.2170 (2.4181)\tTop-5 Accuracy 76.795 (73.542)\t\n",
      "Epoch: [9][5600/5729]\tLoss 2.4336 (2.4186)\tTop-5 Accuracy 72.793 (73.536)\t\n",
      "Epoch: [9][5700/5729]\tLoss 2.5693 (2.4191)\tTop-5 Accuracy 71.864 (73.529)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|█████████████████████████████████████████████████| 40504/40504 [48:36<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Evalaution Metric Scores......\n",
      "\n",
      "loading annotations into memory...\n",
      "0:00:00.607013\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 312143, 'reflen': 313928, 'guess': [312143, 271639, 231135, 190632], 'correct': [202559, 96604, 41486, 17756]}\n",
      "ratio: 0.9943139828240839\n",
      "Bleu_1: 0.645\n",
      "Bleu_2: 0.478\n",
      "Bleu_3: 0.344\n",
      "Bleu_4: 0.248\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.896\n",
      "Epoch 9:\tCIDEr Score: 0.8962054076344467\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Predict: \n",
      "sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, 10):\n",
    "    if epochs_since_improvement == 8:\n",
    "        print('No Improvement for the last 8 epochs. Training terminated')\n",
    "        break\n",
    "    \n",
    "    # Decay\n",
    "    if epoch % 3 == 0 and epoch != 0:\n",
    "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "    \n",
    "    # One epoch training\n",
    "    train(train_loader=train_loader,\n",
    "         encoder=encoder,\n",
    "         decoder=decoder,\n",
    "         criterion=criterion,\n",
    "         encoder_optimizer=encoder_optimizer,\n",
    "         decoder_optimizer=decoder_optimizer,\n",
    "         epoch=epoch,\n",
    "         vocab_size=len(word_map))\n",
    "    \n",
    "    \n",
    "    recent_cider, recent_blue4 = validate(val_loader,\n",
    "                                         encoder,\n",
    "                                         decoder,\n",
    "                                         3,\n",
    "                                         epoch,\n",
    "                                         len(word_map))\n",
    "    print(\"Epoch {}:\\tCIDEr Score: {}\".format(epoch, recent_cider))\n",
    "    \n",
    "    # Check if there was an improvement\n",
    "    #recent_cider = best_cider + 0.01\n",
    "    is_best = recent_cider > best_cider\n",
    "    best_cider = max(recent_cider, best_cider)\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "        \n",
    "    print('Predict: ')\n",
    "    predict_output(\"test1.jpg\", rev_word_map)\n",
    "    \n",
    "\n",
    "    save_checkpoint(epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer, recent_cider, is_best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread('test_imgs/test1.jpg')\n",
    "img = imresize(img, (256, 256)) # (224, 224, 3)\n",
    "img = img.transpose(2, 0, 1) # channel first (3, 224, 224)\n",
    "img = img / 255. # normalize the input to 0 - 1\n",
    "img = torch.FloatTensor(img).to(device) # convert to tensor\n",
    "# normalize the input image\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([normalize])\n",
    "image = transform(img.to('cpu')).to(device)  # (3, 224, 224)\n",
    "\n",
    "# add single batch\n",
    "image = image.unsqueeze(0) # (1, 3, 224, 224)\n",
    "# encoded_image, global_features = encoder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder.vgg(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 8, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(image, rev_word_map):\n",
    "    \"\"\"\n",
    "    predict output with beam size of 1 (predict the word and feet it to the next LSTM)\n",
    "    print out the generated sentence\n",
    "    \"\"\"\n",
    "    max_len = 20\n",
    "    sampled = []\n",
    "    img = imread(image)\n",
    "    img = imresize(img, (256, 256)) # (224, 224, 3)\n",
    "    img = img.transpose(2, 0, 1) # channel first (3, 224, 224)\n",
    "    img = img / 255. # normalize the input to 0 - 1\n",
    "    img = torch.FloatTensor(img).to(device) # convert to tensor\n",
    "    # normalize the input image\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img.to('cpu')).to(device)  # (3, 224, 224)\n",
    "    \n",
    "    # add single batch\n",
    "    image = image.unsqueeze(0) # (1, 3, 224, 224)\n",
    "    encoded_image, global_features = encoder(image)\n",
    "    num_pixels = encoded_image.shape[1]\n",
    "    spatial_image = F.relu(decoder.encoded_to_hidden(encoded_image)) # (batch_size, num_pixels, hidden_size)\n",
    "    global_image = F.relu(decoder.global_features(global_features)) # (batch_size, embed_size)\n",
    "    alphas = torch.zeros(max_len, num_pixels+1)\n",
    "    betas = torch.zeros(max_len, 1)\n",
    "    # create prediction with initial <start> token\n",
    "    predictions = torch.LongTensor([[word_map['<start>']]]).to(device) # (1, 1)\n",
    "    h, c = decoder.init_hidden_state(encoded_image)\n",
    "    \n",
    "    for timestep in range(max_len):\n",
    "        embeddings = decoder.embedding(predictions).squeeze(1) # (1, 1, embed_dim) --> (1, embed_dim)\n",
    "        inputs = torch.cat((embeddings, global_image), dim=1) # (1, embed_dim*2)\n",
    "        h, c, st = decoder.LSTM(inputs, (h, c))\n",
    "        out, alpha, beta = decoder.adaptive_attention(spatial_image, h, st)\n",
    "        pt = decoder.fc(out)\n",
    "        _, pred = pt.max(1)\n",
    "        sampled.append(pred.item())\n",
    "        alphas[timestep] = alpha\n",
    "        betas[timestep] = beta.item()\n",
    "    \n",
    "    generated_words = [rev_word_map[sampled[i]] for i in range(len(sampled))]\n",
    "    filtered_words = ' '.join([word for word in generated_words if word != '<end>'])\n",
    "    print(filtered_words)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer, recent_cider, is_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sebuah sebuah sebuah sebuah bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus bus\n"
     ]
    }
   ],
   "source": [
    "predict_output(\"test1.jpg\", rev_word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9036f7d18ebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder.vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
